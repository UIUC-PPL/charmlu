mainmodule lu {
#if defined(LU_TRACING)
  include "traceToggler.h";
  initproc void traceToggler::registerHandler();
#endif

  readonly CProxy_Main mainProxy;
  readonly int traceTrailingUpdate;
  readonly int traceComputeU;
  readonly int traceComputeL;
  readonly int traceSolveLocalLU;
  readonly CkGroupID mcastMgrGID;

  // Register the reducer function that handles reductions of locval
  initnode void registerLocValReducer();

  message BlockReadyMsg;

  message blkMsg {
    double data[];
    int pes[];
  };

  message UMsg {
    double data[];
  };

  message pivotSequencesMsg {
      int seqIndex[];
      int pivotSequence[];
  };

  message pivotRowsMsg {
      int    rowNum[];
      double rows[];
      double rhs[];
  };

  message rednSetupMsg;

  mainchare Main {
    entry Main(CkArgMsg *m);
    entry void finishInit();
    entry void continueIter();
    entry void arrayIsCreated();
    entry void outputStats();
    entry void calcScaledResidual(CkReductionMsg *);
    entry void startNextStep();
  };

  group LUMgr { };

  group PrioLU {
    entry PrioLU(int BLKSIZE, int matSize);
  };

  include "set";

  class CProxy_BlockScheduler;
  class LUConfig;

  array [2D] LUBlk {
    entry LUBlk(void);
    entry [nokeep] void prepareForPivotRedn(rednSetupMsg *);
    entry void prepareForActivePanel(rednSetupMsg *);
    entry [nokeep] void prepareForRowBeforeDiag(rednSetupMsg *);
    entry [nokeep] void prepareForRowAfterDiag(rednSetupMsg *);
    entry void multicastRedns(int dummy);
    entry void initVec();
    entry void startValidation();
    entry [nokeep] void recvXvec(int size, double xvec[size]);
    entry void sumBvec(int size, double partial_b[size]);

    entry void colMax(CkReductionMsg *m);
    entry void sendPivotData(int rowIndex, int pivotRow, int size, double data[size], double b, bool noop);
    entry [nokeep] void applyPivots(pivotSequencesMsg *msg);
    entry void acceptPivotData(pivotRowsMsg *msg);
    entry [nokeep] void sendUSegment(UMsg* msg);
    entry void performUpdate(CkMarshallMsg *dummyMsg);
    entry void blockUsed(int);
    entry void getBlock(int pe, int rx, int ry);

    entry void belowPivot(int targetCol) {
      // Wait for the replacement row from the diagonal chare so that you have the updated block
      when sendPivotData[internalStep*BLKSIZE + targetCol](int rowIndex, int pivotRow, int size, double data[size],
                                                           double b, bool noop) {
        if (!noop) {
          atomic "below_send_remote_pivot" {
	    ownedPivotThisStep = true;
            CkAssert(pivotRow / BLKSIZE == thisIndex.x);
            CkEntryOptions opts;
            thisProxy(thisIndex.y, thisIndex.y)
              .sendPivotData(internalStep*BLKSIZE + targetCol, pivotRow, BLKSIZE,
                             &LU[getIndex(pivotRow % BLKSIZE,0)], bvec[pivotRow % BLKSIZE], false,
                             &(mgr->setPrio(SEND_PIVOT_DATA, opts)));
          }
          atomic "below_send_useg" {
            UMsg *anUmsg = new(BLKSIZE - targetCol, sizeof(int)*8) UMsg(BLKSIZE - targetCol, &LU[getIndex(pivotRow % BLKSIZE,targetCol)]);
            mgr->setPrio(anUmsg, BELOW_SEND_USEG, internalStep*BLKSIZE + targetCol);
            CkSetRefNum(anUmsg, internalStep*BLKSIZE + targetCol);
            pendingUmsg = (UMsg*)CkCopyMsg((void**)(&anUmsg));
            activePanel.sendUSegment(anUmsg);
          }
          atomic "below_recv_remote_pivot" {
            applySwap(pivotRow % BLKSIZE, 0, data, b);
            thisProxy(thisIndex.x, thisIndex.y).sendUSegment(pendingUmsg);
          }
        }
      }
    };

    // Describes the control flow for all (super, sub and on diagonal) LU chares
    entry void factor() {
      //-----------------------------------------------------------------------
      // Each chare in the array has to process these many trailing updates
      // before the diagonal chare on its row / column becomes active in the factorization process
      //-----------------------------------------------------------------------
      atomic {
        traceOn();
        startTime = CmiWallTimer();
        pivotBatchTag = 0;
      }
      for (internalStep = 0; internalStep < min(thisIndex.x, thisIndex.y); internalStep++) {
          // Receive and process batches of pivot operations sent out by the current active diagonal chare
          while (pivotBatchTag < (internalStep+1)*BLKSIZE) {
              // Wait for msg carrying the next batch of pivot ops
              when applyPivots[pivotBatchTag](pivotSequencesMsg *msg) {
                  // Send out any pivot data owned by this chare
                  atomic "sendOutgoingPivots_trailingChare" {
                    sendPendingPivots(msg);
                    CmiReference(UsrToEnv(msg));
                  }
                  // Wait for all incoming pivot data from this batch
                  while (pendingIncomingPivots > 0)
                  {
                      when acceptPivotData[pivotBatchTag](pivotRowsMsg *aMsg) atomic "acceptIncomingPivots_trailingChare" {
                          for (int i = 0; i < aMsg->nRows; i++)
                          {
                              CkAssert(thisIndex.x == aMsg->rowNum[i]/BLKSIZE);
                              applySwap(aMsg->rowNum[i]%BLKSIZE, 0, &(aMsg->rows[i*BLKSIZE]), aMsg->rhs[i]);
                              pendingIncomingPivots--;
                              VERY_VERBOSE_PIVOT_AGGLOM("[%d,%d] received pivot row %d for batch %d. %d pending\n",
                                  thisIndex.x, thisIndex.y, aMsg->rowNum[i], pivotBatchTag, pendingIncomingPivots);
                          }
                          delete aMsg;
                      }
                  }
                  // Increment the batch tag to the next expected
                  atomic {
                      pivotBatchTag += msg->numRowsProcessed;
                      CmiFree(UsrToEnv(msg));
                  }
              }
          }

	  /// Preserved for RDMA variant
          // overlap {
          //   when readyL[internalStep](BlockReadyMsg *mL) atomic {
          //     DEBUG_PRINT("readyL");
	  //     localScheduler->dataReady(thisIndex, mL);
          //   }
          //   when readyU[internalStep](BlockReadyMsg *mU) atomic {
          //     DEBUG_PRINT("readyU");
          //     localScheduler->dataReady(thisIndex, mU);
          //   }
          // }

          atomic {
            updateExecuted = false;
          }
          while (!updateExecuted) {
            when processTrailingUpdate[internalStep](int step, intptr_t update_ptr)
              atomic "trailing_update" {
	      Update &update = *(Update *)update_ptr;
              if (localScheduler->shouldExecute()) {
                DEBUG_PRINT("doing trailing update");
                updateMatrix(update.L, update.U);
                thisProxy(thisIndex.x, internalStep).blockUsed(1);
                localScheduler->updateDone(update_ptr);
                updateExecuted = true;
              } else {
                update.triggered = false;
                localScheduler->updateUntriggered();
              }
            }
          }
      }

      //-----------------------------------------------------------------------
      // Once the current step of the computation reaches the diagonal chare on your row / column
      //-----------------------------------------------------------------------

      // On chare array diagonal
      if (thisIndex.x == thisIndex.y) atomic {
        if (STOP_AFTER == thisIndex.x) {
            CkStartQD( CkCallback(CkCallback::ckExit) );
        }
        else {
            thisProxy(thisIndex.x, thisIndex.y).processLU();
        }
      }
      // Above diagonal
      else if (thisIndex.x < thisIndex.y) {
        // Receive and process each pivot selection sent out by the current diagonal block
        while (pivotBatchTag < (internalStep+1)*BLKSIZE) {
          // Wait for msg carrying the next batch of pivot ops
          when applyPivots[pivotBatchTag](pivotSequencesMsg *msg) {
              // Send out any pivot data owned by this chare
              atomic "sendOutgoingPivots_activeRowChare" {
                sendPendingPivots(msg);
                CmiReference(UsrToEnv(msg));
              }
              // Wait for all incoming pivot data from this batch
              while (pendingIncomingPivots > 0)
              {
                  when acceptPivotData[pivotBatchTag](pivotRowsMsg *aMsg) atomic "acceptIncomingPivots_activeRowChare" {
                      for (int i = 0; i < aMsg->nRows; i++)
                      {
                          CkAssert(thisIndex.x == aMsg->rowNum[i]/BLKSIZE);
                          applySwap(aMsg->rowNum[i]%BLKSIZE, 0, &(aMsg->rows[i*BLKSIZE]), aMsg->rhs[i]);
                          pendingIncomingPivots--;
                          VERY_VERBOSE_PIVOT_AGGLOM("[%d,%d] received pivot row %d for batch %d. %d pending\n",
                              thisIndex.x, thisIndex.y, aMsg->rowNum[i], pivotBatchTag, pendingIncomingPivots);
                      }
                      delete aMsg;
                  }
              }
              // Increment the batch tag to the next expected
              atomic {
                  pivotBatchTag += msg->numRowsProcessed;
                  CmiFree(UsrToEnv(msg));
              }
          }
        }
        // Once all pivot info is processed, perform the row update
        when recvL[internalStep](blkMsg *mL) atomic "beforeL_schedule_computeU" {
          takeRef(mL);
          L = mL;
          localScheduler->incomingComputeU(thisIndex, internalStep);
        }
      }
      // Below diagonal
      else {
        atomic {
            // Find the pivot candidate for just the first column in the block
            pivotCandidate = findLocVal(0, 0);
            pendingUmsg = NULL;
        }
        // Work in the active column of the chare array proceeds column-by-column through the matrix
        for (activeCol = 0; activeCol < BLKSIZE; activeCol++) {
          atomic "below_find_pivot" {
            // Contribute to a reduction along the active panel to identify the pivot row
            mcastMgr->contribute(sizeof(locval), &pivotCandidate, LocValReducer, pivotCookie);
          }
	  when performUpdate(CkMarshallMsg *dummyMsg) atomic "useg_delayed_update" {
	    // Delayed update using the U segment from the prev activeCol iteration
	    if (pendingUmsg) {
	      updateLsubBlock(activeCol-1, pendingUmsg->data, 2);
	      dropRef(pendingUmsg);
	      pendingUmsg = NULL;
          if (activeCol % 20 == 0) {
              flushLogs();
          }
	    }
	    ownedPivotThisStep = false;
            thisProxy(thisIndex.x, thisIndex.y).belowPivot(activeCol);
        delete dummyMsg;
	  }
          // Wait for the post-diagonal U segment from the original owner of the pivot row
          when sendUSegment(UMsg* anUmsg) atomic "below_recv_useg" {
            // Release the sendPivotData trigger to free memory
	    if (!ownedPivotThisStep)
	      thisProxy(thisIndex).
		sendPivotData(internalStep*BLKSIZE + activeCol, -1, 0, NULL, -1, true);

            if (activeCol == 0) {
              DEBUG_PRINT("startedActivePanel");
              localScheduler->startedActivePanel();
            }
            // Compute multipliers and update just one column to find the next candidate pivot
            pivotCandidate = computeMultipliersAndFindColMax(activeCol, anUmsg->data);
            // Save the Umsg for later use
            pendingUmsg = anUmsg;
            takeRef(pendingUmsg);
          }
        }
        atomic "below_send_L" {
          if (pendingUmsg) dropRef(pendingUmsg);
          DEBUG_PRINT("done computing L");
	  multicastRecvL();
	  factored = true;
	  localScheduler->factorizationDone(thisIndex);
          pivotBatchTag += BLKSIZE;
        }

	while (blockPulled < numBlks-thisIndex.y-1) {
	  when blockUsed(int count) atomic { blockPulled += count; }
	}

        for (internalStep++;  internalStep <= thisIndex.x; internalStep++) {
          // Receive and process batches of pivot operations sent out by the current active diagonal chare
          while (pivotBatchTag < (internalStep+1)*BLKSIZE) {
              // Wait for msg carrying the next batch of pivot ops
              when applyPivots[pivotBatchTag](pivotSequencesMsg *msg) {
                  // Send out any pivot data owned by this chare
                  atomic "sendOutgoingPivots_leftSectionChare" {
                    sendPendingPivots(msg);
                    CmiReference(UsrToEnv(msg));
                  }
                  // Wait for all incoming pivot data from this batch
                  while (pendingIncomingPivots > 0)
                  {
                      when acceptPivotData[pivotBatchTag](pivotRowsMsg *aMsg) atomic "acceptIncomingPivots_leftSectionChare" {
                          for (int i = 0; i < aMsg->nRows; i++)
                          {
                              CkAssert(thisIndex.x == aMsg->rowNum[i]/BLKSIZE);
                              applySwap(aMsg->rowNum[i]%BLKSIZE, 0, &(aMsg->rows[i*BLKSIZE]), aMsg->rhs[i]);
                              pendingIncomingPivots--;
                              VERY_VERBOSE_PIVOT_AGGLOM("[%d,%d] received pivot row %d for batch %d. %d pending\n",
                                  thisIndex.x, thisIndex.y, aMsg->rowNum[i], pivotBatchTag, pendingIncomingPivots);
                          }
                          delete aMsg;
                      }
                  }
                  // Increment the batch tag to the next expected
                  atomic {
                      pivotBatchTag += msg->numRowsProcessed;
                      CmiFree(UsrToEnv(msg));
                  }
              }
          }
        }
      }
      if (thisIndex.x != thisIndex.y) {
        atomic "offdiag_iteration_completed" {
          DEBUG_PRINT("contributing");
          contribute(CkCallback(CkIndex_Main::startNextStep(), mainProxy));
        }
      }
      // Swallow excess pivots message broadcast to the entire array
      while (true) {
        when applyPivots(pivotSequencesMsg *msg) { }
      }
    };

    entry void processLU() {
      atomic {
        CkAssert(pivotBatchTag == thisIndex.x*BLKSIZE);
        pivotRecords.clear();
        numRowsSinceLastPivotSend = 0;
        // Find the pivot candidate for just the first column in the block
        pivotCandidate = findLocVal(0, 0);
        // Toggle projections tracing when necessary
        #if defined(LU_TRACING)
            if ( (cfg.numBlocks > cfg.numTimesToTrace * cfg.numStepsToTrace)
                &&
                 (thisIndex.x % int(cfg.tracePeriodFraction * cfg.numBlocks) == cfg.numStepsToTrace)
               )
            {
                traceToggler::stop();
                DEBUG_PRINT("Suspending projections tracing");
            }
        #endif
      }
      // Process matrix column-by-column
      for (activeCol = 0; activeCol < BLKSIZE; ++activeCol) {
        atomic "diag_find_colmax" {
          VERBOSE_PROGRESS(" ..col%d ", thisIndex.x*BLKSIZE + activeCol);
          // Contribute to a redn along the pivot section to identify the pivot row
          mcastMgr->contribute(sizeof(locval), &pivotCandidate, LocValReducer, pivotCookie);
          if (activeCol % 20 == 0) {
            flushLogs();
          }
        }

        // Share the pivot info with the appropriate array section
        when colMax(CkReductionMsg *m) {
	  atomic "diag_found_colmax" {
            if (activeCol == 0) {
              ckout << "--------------------------------------------------------------------"<<endl
                    << "Block " << thisIndex.x << " queueing local LU at internalStep "
                    << internalStep << ", start time = " << CmiWallTimer() << ", time = "
                    << CmiWallTimer() - startTime << endl;
              DEBUG_PRINT("startedActivePanel");
              localScheduler->startedActivePanel();
            }
            DEBUG_PRINT("colMax happened");
	    pivotCandidate = *(locval *)(m->getData());
            delete m;
	  }
	  if (pivotCandidate.loc / BLKSIZE == thisIndex.x) {
	    // Now, send the post-diagonal portion of the pivot row (a segment of U) to all active panel brethren
	    // so that they can start updating their sub-blocks right away
	    atomic "diag_send_useg" {
	      UMsg *anUmsg = new(BLKSIZE - activeCol, sizeof(int)*8) UMsg(BLKSIZE - activeCol, &LU[getIndex(pivotCandidate.loc%BLKSIZE,activeCol)]);
              mgr->setPrio(anUmsg, DIAG_SEND_USEG);
	      activePanel.sendUSegment(anUmsg);
	    }
	    // Perform the local pivoting
	    atomic "diag_local_pivot" { swapLocal(activeCol, pivotCandidate.loc % BLKSIZE); }
	  } else {
	    // Send pivot data to remote chare in active panel
	    atomic "diag_send_remote_pivot" {
	      // Trigger swap of data
	      CkEntryOptions opts;
	      thisProxy(pivotCandidate.loc / BLKSIZE, thisIndex.y)
		.sendPivotData(internalStep*BLKSIZE + activeCol, pivotCandidate.loc, BLKSIZE,
			       &LU[getIndex((activeCol + BLKSIZE * thisIndex.y) % BLKSIZE,0)],
			       bvec[(activeCol + BLKSIZE * thisIndex.y) % BLKSIZE], false,
                               &(mgr->setPrio(DIAG_SEND_PIVOT, opts)));
	    }
	    when sendPivotData[internalStep*BLKSIZE + activeCol](int rowIndex, int pivotRow, int size, double data[size], double b, bool noop) atomic "diag_receive_remote_pivot" {
              CkAssert(!noop);
	      applySwap(activeCol, 0, data, b);
	    }
	  }
        }

        atomic "diag_agglomerate_pivots" {
            // Store the pivot information
            recordPivot(thisIndex.y*BLKSIZE + activeCol, pivotCandidate.loc);
            // at opportune steps, send out the agglomerated pivot ops
            if ( shouldSendPivots() || activeCol == BLKSIZE-1)
                announceAgglomeratedPivots();
        }

        /// Compute the multipliers and update the trailing sub-block
        atomic "diag_update" {
            // Pivoting is done, so the diagonal entry better not be zero; else the matrix is singular
            if (fabs(LU[getIndex(activeCol,activeCol)]) <= 100 * std::numeric_limits<double>::epsilon() )
                CkAbort("Diagonal element very small despite pivoting. Is the matrix singular??");
            // Compute the multipliers and find the next local pivot candidate
            pivotCandidate = computeMultipliersAndFindColMax(activeCol, &LU[getIndex(activeCol,activeCol)], activeCol+1);
            // Update the sub-block
            updateLsubBlock(activeCol, &LU[getIndex(activeCol,activeCol)], 2, activeCol+1);
        }
      }

      atomic "diag_multicast_L" {
        ckout << "Block " << thisIndex.x << " finished local LU at internalStep "
              << internalStep << ", time = " << CmiWallTimer() - startTime << endl;
        // Toggle projections tracing when necessary
        #if defined(LU_TRACING)
            if ((thisIndex.x + 1) % int(cfg.tracePeriodFraction * cfg.numBlocks) == 0 || (cfg.numBlocks -1 - thisIndex.x) == cfg.numStepsToTrace)
            {
                traceToggler::start();
                DEBUG_PRINT("Resuming projections tracing");
            }
        #endif

        localScheduler->factorizationDone(thisIndex);
        factored = true;
        // All diagonal chares except the last must continue the factorization
        if (thisIndex.x < numBlks-1 && thisIndex.y < numBlks-1) {
          DEBUG_PRINT("is top,left block this step, multicast L");
          // Multicast the local block of L rightwards to the blocks in the same row
          multicastRecvL();
        }
        // Contribute to a reduction indicating that this chare is done with its factorization work
	DEBUG_PRINT("contributing");
        contribute(CkCallback(CkIndex_Main::startNextStep(), mainProxy));
        VERBOSE_PROGRESS("\n");
      }
    };

    entry void beginForward(int size, double preVec[size]);
    entry void beginBackward(int size, double preVec[size]);
    entry void recvSolveData(CkReductionMsg *m);
    entry void schedulerReady(CkReductionMsg *m);

    entry void startup(const LUConfig luCfg, CProxy_LUMgr _mgr, CProxy_BlockScheduler bs) {
      when schedulerReady(CkReductionMsg *m) atomic {
	delete m;
	init(luCfg, _mgr, bs);
      }
      when multicastRedns(int dummy) {
        if (thisIndex.x >= thisIndex.y) {
          when prepareForPivotRedn(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(pivotCookie, msg);
            mcastMgr->setReductionProgressEvent(pivotCookie, CkCallback(CkIndex_LUBlk::performUpdate(0), CkArrayIndex2D(thisIndex.x,thisIndex.y), thisArrayID) );
          }
        }
        if (thisIndex.x > thisIndex.y) {
          when prepareForRowBeforeDiag(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(rowBeforeCookie, msg);
          }
        } else if (thisIndex.x < thisIndex.y) {
          when prepareForRowAfterDiag(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(rowAfterCookie, msg);
          }
        }
        atomic { contribute(CkCallback(CkIndex_Main::finishInit(), mainProxy)); }
      }
    };

    entry void forwardSolve() {
        if (thisIndex.x > 0) {
          // All except the first diag chare have to wait for their row reductions
          when recvSolveData[thisIndex.x](CkReductionMsg *m) atomic {
            double *preVec = (double*) ( m->getData() );
            for (int i = 0; i < BLKSIZE; i++)
              bvec[i] -= preVec[i];
            delete m;
          }
        }

        atomic {
          localForward(bvec);
          if (thisIndex.x == numBlks-1)
            thisProxy(thisIndex.x, thisIndex.y).backwardSolve();
          else {
            // Broadcast downward from diagonal beginForward
            CProxySection_LUBlk col = CProxySection_LUBlk::ckNew(thisArrayID,
                                                    thisIndex.x+1, numBlks-1, 1,
                                                    thisIndex.y, thisIndex.y, 1);
            col.beginForward(BLKSIZE, bvec);
            thisProxy(thisIndex.x+1,thisIndex.y+1).forwardSolve();
          }
        }
      };

      entry void backwardSolve() {
        if (thisIndex.x < numBlks-1) {
          // All except the last diag chare have to wait for their row reductions
          when recvSolveData[thisIndex.x](CkReductionMsg *m) atomic {
            double *preVec = (double*) ( m->getData() );
            for (int i = 0; i < BLKSIZE; i++)
              bvec[i] -= preVec[i];
            delete m;
          }
        }

        atomic {
          localBackward(bvec);
          if (thisIndex.x == 0)
            mainProxy.startNextStep();
          else {
            // Broadcast upward from diagonal beginBackward
            CProxySection_LUBlk col = CProxySection_LUBlk::ckNew(thisArrayID,
                                                    0, thisIndex.x-1, 1,
                                                    thisIndex.y, thisIndex.y, 1);
            col.beginBackward(BLKSIZE, bvec);
            thisProxy(thisIndex.x-1, thisIndex.y-1).backwardSolve();
          }
        }
      };

    // These entry methods are the targets of the row- and column-wise
    // multicasts each block makes.
    //
    // These methods reference but will not modify the messages
    // delivered to them. Thus, we can use the [nokeep] annotation to
    // tell the runtime that it can safely deliver the same message
    // instance to every object on a processor, rather than making a
    // separate copy for each object.
    entry [nokeep] void recvL(blkMsg *);

    //entry void readyL(BlockReadyMsg *mL);
    //entry void readyU(BlockReadyMsg *mU);

    // These process* entry methods are invoked locally by each block
    // on itself with varying priorities, to let the scheduler decide
    // when the work they represent (the bulk of the computation)
    // should execute.
    //
    // When a block computes a trailing update, it no longer needs to
    // retain the incoming data from the corresponding row and column
    // updates. The [memcritical] annotation tells the runtime that it
    // should schedule invocations of this entry method when it is in
    // a memory critical (i.e. over threshold) condition, so that
    // retained messages can be freed.
    entry void processTrailingUpdate(int step, intptr_t update_ptr);
    // This frees a buffered message, but don't really reduce
    // memory pressure, because it generates multicasts along the
    // way. Hence, it's not marked [memcritical].
    entry void processComputeU(int ignoredParam);

    entry void flushLogs();
    entry void print();

    entry void finishInit();
  };

  group OnePerPE : CkArrayMap {
    entry OnePerPE();
  };

  array [1D] BlockScheduler {
    entry BlockScheduler(CProxy_LUBlk luArr, LUConfig config, CProxy_LUMgr mgr_);
    entry void deliverBlock(blkMsg *m);
    entry void printBlockLimit();
    entry void allRegistered(CkReductionMsg *m);
  }

  group LUMap : CkArrayMap {
    entry LUMap();
  };

  group BlockCyclicMap : LUMap {
    entry BlockCyclicMap();
  };

  group RealBlockCyclicMap : LUMap {
      entry RealBlockCyclicMap(int r, int num_blocks);
  };

  group LUSnakeMap : LUMap {
    entry LUSnakeMap(int, int);
  };

  group LUBalancedSnakeMap : LUMap {
    entry LUBalancedSnakeMap(int, int);
  };

  group LUBalancedSnakeMap2 : LUMap {
    entry LUBalancedSnakeMap2(int, int);
  };

  group PE2DTilingMap: LUMap {
    entry PE2DTilingMap(int _peRows, int _peCols, int _peRotate);
  };

  group StrongScaling1: LUMap {
    entry StrongScaling1(int numBlks_);
  };
};

