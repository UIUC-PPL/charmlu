module lu {
  extern module luUtils;

  message blkMsg {
    double data[];
    int pes[];
  };

  message UMsg {
    double data[];
  };

  message BVecMsg {
    double data[];
  };

  message pivotSequencesMsg {
    int seqIndex[];
    int pivotSequence[];
  };

  message pivotRowsMsg {
    int rowNum[];
    double rows[];
    double rhs[];
  };

  /**
   * 2D array of LU matrix-block objects that are locally coordinated
   * in Structured Dagger
   */
  array [2D] LUBlk {
    /// Constructor
    entry LUBlk(void);

    /// Each block starts its participation in the factorization here
    entry void factor() {
      // Keep executing trailing updates on this block until it is on the active
      // block row or column
      atomic { thisProxy[thisIndex].updateUntilActive(); }
      // Once the current step of the computation reaches the diagonal chare on this block's row / column
      when amActive(bool isActive) {
        // If this matrix block is above the matrix diagonal
        if      (isAboveDiagonal()) atomic { thisProxy[thisIndex].factorAboveDiagBlock(); }
        // else if its below the matrix diagonal
        else if (isBelowDiagonal()) atomic { thisProxy[thisIndex].factorBelowDiagBlock(); }
        // else, this block is on the matrix diagonal
        else                        atomic { thisProxy[thisIndex].factorDiagBlock(); }
      }
    };

    /**
     * Active panel methods
     */
    // Block is ready for active panel work
    entry void amActive(bool isActive);
    // The reduction to find the maximum element in the column
    entry void colMax(CkReductionMsg *m);
    // Pivot data to swap along with row index
    entry void pivotRowSwap(int rowIndex, int pivotRow, int len, const double data[len], double b, bool noop);
    // U-Segment for the next computation
    entry [nokeep] void USegCompute(UMsg* msg);
    // Using the U-Segment previously received, update the sub-block
    entry void updateSubBlock(CkMarshallMsg *dummyMsg);

    /**
     * Methods for handling trailing pivots
     */
    entry [nokeep] void applyTrailingPivots(pivotSequencesMsg *msg);
    entry void trailingPivotRowsSwap(pivotRowsMsg *msg);

    // Describes the control flow for all (super, sub and on diagonal) LU chares
    entry [local] void updateUntilActive() {
      //-----------------------------------------------------------------------
      // Each chare in the array has to process these many trailing updates
      // before the diagonal chare on its row / column becomes active in the factorization process
      //-----------------------------------------------------------------------
      atomic {
        startTime = CmiWallTimer();
        pivotBatchTag = 0;
      }
      for (internalStep = 0; internalStep < min(thisIndex.x, thisIndex.y); internalStep++) {
        // Receive and process batches of pivot operations sent out by the current active diagonal chare
        while (pivotBatchTag < (internalStep+1)*blkSize) {
          // Wait for msg carrying the next batch of pivot ops
          when applyTrailingPivots[pivotBatchTag](pivotSequencesMsg *msg) {
            // Send out any pivot data owned by this chare
            atomic "sendOutgoingPivots_trailingChare" {
              sendPendingPivots(msg);
              CmiReference(UsrToEnv(msg));
            }
            // Wait for all incoming pivot data from this batch
            while (pendingIncomingPivots > 0) {
              when trailingPivotRowsSwap[pivotBatchTag](pivotRowsMsg *aMsg) atomic "acceptIncomingPivots_trailingChare" {
                for (int i = 0; i < aMsg->nRows; i++) {
                  CkAssert(thisIndex.x == aMsg->rowNum[i]/blkSize);
                  applySwap(aMsg->rowNum[i]%blkSize, 0, &(aMsg->rows[i*blkSize]), aMsg->rhs[i]);
                  pendingIncomingPivots--;
                  VERY_VERBOSE_PIVOT_AGGLOM("[%d,%d] received pivot row %d for batch %d. %d pending\n",
                                            thisIndex.x, thisIndex.y, aMsg->rowNum[i], pivotBatchTag, pendingIncomingPivots);
                }
                delete aMsg;
              }
            }
            // Increment the batch tag to the next expected
            atomic {
              pivotBatchTag += msg->numRowsProcessed;
              CmiFree(UsrToEnv(msg));
            }
          }
        }

        atomic { updateExecuted = false; }
        while (!updateExecuted) {
          when processTrailingUpdate[internalStep](int step, intptr_t update_ptr)
          atomic "trailing_update" {
            Update &update = *(Update *)update_ptr;
            if (localScheduler->shouldExecute()) {
              DEBUG_PRINT("Executing trailing update");
              updateMatrix(update.L, update.U);
              thisProxy(thisIndex.x, internalStep).blockUsed(1);
              localScheduler->updateDone(update_ptr);
              updateExecuted = true;
            } else {
              update.triggered = false;
              localScheduler->updateUntriggered();
            }
          }
        }
      }
      atomic { thisProxy[thisIndex].amActive(true); }
    };

    entry [local] void factorAboveDiagBlock() {
      // Receive and process each pivot selection sent out by the current diagonal block
      while (pivotBatchTag < (internalStep+1)*blkSize) {
        // Wait for msg carrying the next batch of pivot ops
        when applyTrailingPivots[pivotBatchTag](pivotSequencesMsg *msg) {
          // Send out any pivot data owned by this chare
          atomic "sendOutgoingPivots_activeRowChare" {
            sendPendingPivots(msg);
            CmiReference(UsrToEnv(msg));
          }
          // Wait for all incoming pivot data from this batch
          while (pendingIncomingPivots > 0) {
            when trailingPivotRowsSwap[pivotBatchTag](pivotRowsMsg *aMsg) atomic "acceptIncomingPivots_activeRowChare" {
              for (int i = 0; i < aMsg->nRows; i++) {
                CkAssert(thisIndex.x == aMsg->rowNum[i]/blkSize);
                applySwap(aMsg->rowNum[i]%blkSize, 0, &(aMsg->rows[i*blkSize]), aMsg->rhs[i]);
                pendingIncomingPivots--;
                VERY_VERBOSE_PIVOT_AGGLOM("[%d,%d] received pivot row %d for batch %d. %d pending\n",
                                          thisIndex.x, thisIndex.y, aMsg->rowNum[i], pivotBatchTag, pendingIncomingPivots);
              }
              delete aMsg;
            }
          }
          // Increment the batch tag to the next expected
          atomic {
            pivotBatchTag += msg->numRowsProcessed;
            CmiFree(UsrToEnv(msg));
          }
        }
      }

      if (thisIndex.y == thisIndex.x + 1) {
        when triangularSolve(blkMsg* m) atomic "triangular_solve" {
          DEBUG_PRINT("Executing triangular solve");
          computeU(m->data);
          delete m;
          scheduler.releaseActiveColumn(thisIndex.y, thisIndex.x);
        }
      } else {
        atomic { updateExecuted = false; }
        while (!updateExecuted) {
          when processComputeU(intptr_t update_ptr) atomic "process_compute_U" {
            Update &update = *(Update *)update_ptr;
            if (localScheduler->shouldExecute()) {
              DEBUG_PRINT("Executing triangular solve");
              computeU(update.L);
              localScheduler->updateDone(update_ptr);
              updateExecuted = true;
              scheduler.releaseActiveColumn(thisIndex.y, thisIndex.x);
            } else {
              update.triggered = false;
              localScheduler->updateUntriggered();
            }
          }
        }
      }
      atomic "finish_U" {
        scheduleDownwardU();
        factored = true;
        localScheduler->factorizationDone(thisIndex);
      }
      atomic "abovediag_iteration_completed" { contribute(factorizationDone); }
      // Swallow excess pivots message broadcast to the entire array
      while (true) { when applyTrailingPivots(pivotSequencesMsg *msg) { } }
    };

    // Below diagonal
    entry [local] void factorBelowDiagBlock() {
      atomic {
        // Find the pivot candidate for just the first column in the block
        pivotCandidate = findMaxElm(0, 0);
        pendingUmsg = NULL;
      }
      // Work in the active column of the chare array proceeds column-by-column through the matrix
      for (activeCol = 0; activeCol < blkSize; activeCol++) {
        atomic "below_find_pivot" {
          // Contribute to a reduction along the active panel to identify the pivot row
          mcastMgr->contribute(sizeof(MaxElm), &pivotCandidate, MaxElmReducer, pivotCookie);
#ifndef SCHED_PIVOT_REDN
          void *dummyMsg = CkAllocSysMsg();
          thisProxy[thisIndex].updateSubBlock((CkMarshallMsg*)dummyMsg);
#endif
        }
        when updateSubBlock(CkMarshallMsg *dummyMsg) atomic "useg_delayed_update" {
          // Delayed update using the U segment from the prev activeCol iteration
          if (pendingUmsg) {
            updateLsubBlock(activeCol-1, pendingUmsg->data, 2);
            dropRef(pendingUmsg);
            pendingUmsg = NULL;
            if (activeCol % 20 == 0) {
              flushLogs();
            }
          }
          ownedPivotThisStep = false;
          thisProxy[thisIndex].belowPivot(activeCol);
          delete dummyMsg;
        }
        // Wait for the post-diagonal U segment from the original owner of the pivot row
        when USegCompute(UMsg* anUmsg) atomic "below_recv_useg" {
          // Release the pivotRowSwap trigger to free memory
          if (!ownedPivotThisStep)
            thisProxy(thisIndex).pivotRowSwap(internalStep*blkSize + activeCol, -1, 0, NULL, -1, true);
          if (activeCol == 0) {
            localScheduler->startedActivePanel();
          }
          // Compute multipliers and update just one column to find the next candidate pivot
          pivotCandidate = computeMultipliersAndFindColMax(activeCol, anUmsg->data);
          // Save the Umsg for later use
          pendingUmsg = anUmsg;
          takeRef(pendingUmsg);
        }
      }
      atomic "below_send_L" {
        if (pendingUmsg) dropRef(pendingUmsg);
        DEBUG_PRINT("Finished computing L");
        scheduleRightwardL();
        factored = true;
        localScheduler->factorizationDone(thisIndex);
        pivotBatchTag += blkSize;
      }

      while (blockPulled < numBlks-thisIndex.y-1) {
        when blockUsed(int count) atomic { blockPulled += count; }
      }

      for (internalStep++; internalStep <= thisIndex.x; internalStep++) {
        // Receive and process batches of pivot operations sent out by the current active diagonal chare
        while (pivotBatchTag < (internalStep+1)*blkSize) {
          // Wait for msg carrying the next batch of pivot ops
          when applyTrailingPivots[pivotBatchTag](pivotSequencesMsg *msg) {
            // Send out any pivot data owned by this chare
            atomic "sendOutgoingPivots_leftSectionChare" {
              sendPendingPivots(msg);
              CmiReference(UsrToEnv(msg));
            }
            // Wait for all incoming pivot data from this batch
            while (pendingIncomingPivots > 0) {
              when trailingPivotRowsSwap[pivotBatchTag](pivotRowsMsg *aMsg) atomic "acceptIncomingPivots_leftSectionChare" {
                for (int i = 0; i < aMsg->nRows; i++) {
                  CkAssert(thisIndex.x == aMsg->rowNum[i]/blkSize);
                  applySwap(aMsg->rowNum[i]%blkSize, 0, &(aMsg->rows[i*blkSize]), aMsg->rhs[i]);
                  pendingIncomingPivots--;
                  VERY_VERBOSE_PIVOT_AGGLOM("[%d,%d] received pivot row %d for batch %d. %d pending\n",
                                            thisIndex.x, thisIndex.y, aMsg->rowNum[i], pivotBatchTag, pendingIncomingPivots);
                }
                delete aMsg;
              }
            }
            // Increment the batch tag to the next expected
            atomic {
              pivotBatchTag += msg->numRowsProcessed;
              CmiFree(UsrToEnv(msg));
            }
          }
        }
      }
      atomic "belowdiag_iteration_completed" { contribute(factorizationDone); }
      // Swallow excess pivots message broadcast to the entire array
      while (true) { when applyTrailingPivots(pivotSequencesMsg *msg) { } }
    };

    entry void factorDiagBlock() {
      atomic {
        CkAssert(pivotBatchTag == thisIndex.x*blkSize);
        pivotRecords.clear();
        numRowsSinceLastPivotSend = 0;
        // Find the pivot candidate for just the first column in the block
        pivotCandidate = findMaxElm(0, 0);
      }
      // Process matrix column-by-column
      for (activeCol = 0; activeCol < blkSize; ++activeCol) {
        atomic "diag_find_colmax" {
          VERBOSE_PROGRESS(" ..col%d ", thisIndex.x*blkSize + activeCol);
          // Contribute to a redn along the pivot section to identify the pivot row
          mcastMgr->contribute(sizeof(MaxElm), &pivotCandidate, MaxElmReducer, pivotCookie);
          if (activeCol % 20 == 0) {
            flushLogs();
          }
        }

        // Share the pivot info with the appropriate array section
        when colMax(CkReductionMsg *m) {
	  atomic "diag_found_colmax" {
            if (activeCol == 0) {
              ckout << "--------------------------------------------------------------------"<<endl
                    << "Block " << thisIndex.x << " queueing local LU at internalStep "
                    << internalStep << ", start time = " << CmiWallTimer() << ", time = "
                    << CmiWallTimer() - startTime << endl;
              DEBUG_PRINT("startedActivePanel");
              localScheduler->startedActivePanel();
            }
            DEBUG_PRINT("colMax %d happened", activeCol);
	    pivotCandidate = *(MaxElm *)(m->getData());
            delete m;
	  }
	  if (pivotCandidate.loc / blkSize == thisIndex.x) {
	    // Now, send the post-diagonal portion of the pivot row (a segment of U) to all active panel brethren
	    // so that they can start updating their sub-blocks right away
	    atomic "diag_send_useg" {
	      UMsg *anUmsg = new(blkSize - activeCol, sizeof(int)*8) UMsg(blkSize - activeCol, &LU[getIndex(pivotCandidate.loc%blkSize,activeCol)]);
              mgr->setPrio(anUmsg, DIAG_SEND_USEG);
	      activePanel.USegCompute(anUmsg);
	    }
	    // Perform the local pivoting
	    atomic "diag_local_pivot" { swapLocal(activeCol, pivotCandidate.loc % blkSize); }
	  } else {
	    // Send pivot data to remote chare in active panel
	    atomic "diag_send_remote_pivot" {
	      // Trigger swap of data
	      CkEntryOptions opts;
	      thisProxy(pivotCandidate.loc / blkSize, thisIndex.y)
		.pivotRowSwap(internalStep*blkSize + activeCol, pivotCandidate.loc, blkSize,
			      &LU[getIndex((activeCol + blkSize * thisIndex.y) % blkSize,0)],
			      bvec[(activeCol + blkSize * thisIndex.y) % blkSize], false,
                              &(mgr->setPrio(DIAG_SEND_PIVOT, opts)));
	    }
	    when pivotRowSwap[internalStep*blkSize + activeCol](int rowIndex, int pivotRow, int len,
                                                                const double data[len], double b, bool noop)
            atomic "diag_receive_remote_pivot" {
              CkAssert(!noop);

#if !defined(CHARMLU_USEG_FROM_BELOW)
	      UMsg *anUmsg = new(blkSize - activeCol, sizeof(int)*8) UMsg(blkSize - activeCol, data + activeCol);
              mgr->setPrio(anUmsg, DIAG_SEND_USEG);
	      activePanel.USegCompute(anUmsg);
#endif
	      applySwap(activeCol, 0, data, b);
	    }
	  }
        }

        atomic "diag_agglomerate_pivots" {
          // Store the pivot information
          recordPivot(thisIndex.y*blkSize + activeCol, pivotCandidate.loc);
          // at opportune steps, send out the agglomerated pivot ops
          if (shouldSendPivots() || activeCol == blkSize-1)
            announceAgglomeratedPivots();
        }

        /// Compute the multipliers and update the trailing sub-block
        atomic "diag_update" {
          // Pivoting is done, so the diagonal entry better not be zero; else the matrix is singular
          if (fabs(LU[getIndex(activeCol,activeCol)]) <= 100 * std::numeric_limits<double>::epsilon() )
            CkAbort("Diagonal element very small despite pivoting. Is the matrix singular??");
          // Compute the multipliers and find the next local pivot candidate
          pivotCandidate = computeMultipliersAndFindColMax(activeCol, &LU[getIndex(activeCol,activeCol)], activeCol+1);
          // Update the sub-block
          updateLsubBlock(activeCol, &LU[getIndex(activeCol,activeCol)], 2, activeCol+1);
        }
      }

      atomic "diag_multicast_L" {
        ckout << "Block " << thisIndex.x << " finished local LU at internalStep "
              << internalStep << ", time = " << CmiWallTimer() - startTime << endl;

        localScheduler->factorizationDone(thisIndex);
        factored = true;
        // All diagonal chares except the last must continue the factorization
        if (thisIndex.x < numBlks-1 && thisIndex.y < numBlks-1) {
          DEBUG_PRINT("is top,left block this step, multicast L");
          if (isOnDiagonal()) {
            blkMsg *msg = new (blkSize * blkSize, 0, mgr->bitsOfPrio()) blkMsg(thisIndex);
            memcpy(msg->data, LU, sizeof(double) * blkSize * blkSize);
            thisProxy(thisIndex.x, thisIndex.y + 1).triangularSolve(msg);
          }
          // Multicast the local block of L rightwards to the blocks in the same row
          scheduleRightwardL();
        }
        // Contribute to a reduction indicating that this chare is done with its factorization work
	DEBUG_PRINT("contributing");
        contribute(factorizationDone);
        VERBOSE_PROGRESS("\n");
      }
    };

    entry void belowPivot(int targetCol) {
      // Wait for the replacement row from the diagonal chare so that you have the updated block
      when pivotRowSwap[internalStep*blkSize + targetCol](int rowIndex, int pivotRow, int len, double data[len],
                                                          double b, bool noop) {
        if (!noop) {
          atomic "below_send_remote_pivot" {
	    ownedPivotThisStep = true;
            CkAssert(pivotRow / blkSize == thisIndex.x);
            CkEntryOptions opts;
            thisProxy(thisIndex.y, thisIndex.y)
              .pivotRowSwap(internalStep*blkSize + targetCol, pivotRow, blkSize,
                            &LU[getIndex(pivotRow % blkSize,0)], bvec[pivotRow % blkSize], false,
                            &(mgr->setPrio(SEND_PIVOT_DATA, opts)));
          }
          atomic "below_send_useg" {
#if defined(CHARMLU_USEG_FROM_BELOW)
            UMsg *anUmsg = new(blkSize - targetCol, sizeof(int)*8) UMsg(blkSize - targetCol, &LU[getIndex(pivotRow % blkSize,targetCol)]);
            mgr->setPrio(anUmsg, BELOW_SEND_USEG, internalStep*blkSize + targetCol);
            CkSetRefNum(anUmsg, internalStep*blkSize + targetCol);
            pendingUmsg = (UMsg*)CkCopyMsg((void**)(&anUmsg));
            activePanel.USegCompute(anUmsg);
#endif
          }
          atomic "below_recv_remote_pivot" {
            applySwap(pivotRow % blkSize, 0, data, b);
#if defined(CHARMLU_USEG_FROM_BELOW)
            thisProxy[thisIndex].USegCompute(pendingUmsg);
#endif
          }
        }
      }
    };

    entry [nokeep] void offDiagSolve(BVecMsg *m);
    entry void recvSolveData(CkReductionMsg *m);
    entry void schedulerReady(CkReductionMsg *m);
    entry void dataReady(int);

    entry void startup(const LUConfig luCfg, CProxy_LUMgr _mgr,
                       CProxy_BlockScheduler bs, CkCallback initialized, CkCallback factored, CkCallback solved) {
      when schedulerReady(CkReductionMsg *m) atomic {
	delete m;
	init(luCfg, _mgr, bs, initialized, factored, solved);
      }
      when multicastRedns(int dummy) {
        if (isOnDiagonal() || isBelowDiagonal()) {
          when prepareForPivotRedn(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(pivotCookie, msg);
#ifdef SCHED_PIVOT_REDN
            mcastMgr->setReductionProgressEvent(pivotCookie, CkCallback(CkIndex_LUBlk::updateSubBlock(0), CkArrayIndex2D(thisIndex.x,thisIndex.y), thisArrayID) );
#endif
          }
        }
        if (isBelowDiagonal()) {
          when prepareForRowBeforeDiag(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(rowBeforeCookie, msg);
          }
        } else if (isAboveDiagonal()) {
          when prepareForRowAfterDiag(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(rowAfterCookie, msg);
          }
        }
	when dataReady(int foo) atomic {
	  contribute(initDone);
	}
      }
    };

    entry void forwardSolve() {
      if (thisIndex.x > 0) {
        // All except the first diag chare have to wait for their row reductions
        when recvSolveData[thisIndex.x](CkReductionMsg *m) atomic {
          double *preVec = (double*) ( m->getData() );
          for (int i = 0; i < blkSize; i++)
            bvec[i] -= preVec[i];
          delete m;
        }
      }
      atomic {
        localForward(bvec);
        if (thisIndex.x != numBlks-1) {
          // Broadcast downward from diagonal
          BVecMsg *m = new (blkSize) BVecMsg(blkSize, bvec, true);
          activePanel.offDiagSolve(m);
        }
        thisProxy(thisIndex).backwardSolve();
      }
    };

    entry void backwardSolve() {
      if (thisIndex.x < numBlks-1) {
        // All except the last diag chare have to wait for their row reductions
        when recvSolveData[thisIndex.x](CkReductionMsg *m) atomic {
          double *preVec = (double*) ( m->getData() );
          for (int i = 0; i < blkSize; i++)
            bvec[i] -= preVec[i];
          delete m;
        }
      }

      atomic {
        localBackward(bvec);
        if (thisIndex.x == 0) {
          solveDone.send();
        }
        else {
          // Broadcast upward from diagonal
          CProxySection_LUBlk col = CProxySection_LUBlk::ckNew(thisArrayID,
                                                               0, thisIndex.x-1, 1,
                                                               thisIndex.y, thisIndex.y, 1);
          col.ckSectionDelegate(mcastMgr);
          BVecMsg *m = new (blkSize) BVecMsg(blkSize, bvec, false);
          col.offDiagSolve(m);
        }
      }
    };

    // These process* entry methods are invoked locally on each block
    // by the BlockScheduler with varying priorities, to let the
    // scheduler decide when the work they represent (the bulk of the
    // computation) should execute.
    entry void processTrailingUpdate(int step, intptr_t update_ptr);
    entry void processComputeU(intptr_t update_ptr);

    // Method for delivering triangular solve data
    entry void triangularSolve(blkMsg* m);

    // Inform the BlockScheduler when a block for a trailing update has be consumed
    entry void blockUsed(int);
    // Ask the BlockScheduler for a certain block for a trailing update
    entry void requestBlock(int pe, int rx, int ry);

    entry void flushLogs();

    entry [nokeep] void prepareForPivotRedn(rednSetupMsg *);
    entry [nokeep] void prepareForActivePanel(rednSetupMsg *);
    entry [nokeep] void prepareForRowBeforeDiag(rednSetupMsg *);
    entry [nokeep] void prepareForRowAfterDiag(rednSetupMsg *);
    entry void multicastRedns(int dummy);

  };

  array [1D] BlockScheduler {
    entry BlockScheduler(CProxy_LUBlk luArr, LUConfig config, CProxy_LUMgr mgr_);
    entry void deliverBlock(blkMsg *m);
    entry void printBlockLimit();
    entry void allRegistered(CkReductionMsg *m);
    entry void scheduleSend(CkIndex2D index, bool onActive);
    entry void releaseActiveColumn(const int y, const int t);
    entry void outputStats();
  }
};

