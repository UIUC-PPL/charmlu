module lu {
  extern module luUtils;
  extern module luMessages;
  include "maxelm.h";

  /**
   * 2D array of LU matrix-block objects that are locally coordinated
   * in Structured Dagger.
   *
   * The chare indexing relates to matrix position as follows:
   *           y
   *    o------------>
   *    | (0,0) (0,1) . . .
   *    | (1,0) (1,1)
   *  x |   .         .
   *    |   .           .
   *    |   .             .
   *    v
   */
  array [2D] LUBlk {
    /// Constructor
    entry LUBlk(void);

    /// Each block starts its participation in the factorization here
    entry void factor() {
      // Keep executing trailing updates on this block until it is on
      // the active block row or column
      atomic { thisProxy[thisIndex].updateUntilActive(); }
      // Once the current step of the computation reaches the diagonal
      // chare on this block's row / column
      when amActive(bool isActive) {
        if      (isAboveDiagonal()) atomic { thisProxy[thisIndex].factorAboveDiagBlock(); }
        else if (isBelowDiagonal()) atomic { thisProxy[thisIndex].factorBelowDiagBlock(); }
        else     /* on diagonal */  atomic { thisProxy[thisIndex].factorDiagBlock(); }
      }
    };

    // Describes the control flow of alternating pivoting and trailing
    // updates for all LUBlk chares before they become part of the
    // active panel.
    entry [local] void updateUntilActive() {
      //-----------------------------------------------------------------------
      // Each chare in the array has to process these many trailing
      // updates before the diagonal chare on its row / column becomes
      // active in the factorization process.
      // -----------------------------------------------------------------------
      atomic {
        startTime = CmiWallTimer();
        pivotBatchTag = 0;
      }
      for (internalStep = 0; internalStep < min(thisIndex.x, thisIndex.y); internalStep++) {
        atomic { thisProxy[thisIndex].pivotBatch(0); }

        // After a panel's worth of pivots have arrived, this block is
        // ready to execute a trailing update. We let the local
        // scheduler dictate when that should happen, to let it
        // control memory consumption and avoid interfering with
        // another local block working in the latency-sensitive active
        // panel.
        when pivotBatchDone[0](int tag) atomic { updateExecuted = false; }
        while (!updateExecuted) {
          when processTrailingUpdate[internalStep](int step, intptr_t update_ptr)
          atomic "trailing_update" {
            Update &update = *(Update *)update_ptr;
            if (localScheduler->shouldExecute()) {
              updateMatrix(update.L, update.U);
              thisProxy(thisIndex.x, internalStep).blockUsed(1);
              localScheduler->updateDone(update_ptr);
              updateExecuted = true;
            } else {
              update.triggered = false;
              localScheduler->updateUntriggered();
            }
          }
        }
      }
      atomic { thisProxy[thisIndex].amActive(true); }
    };
    entry void amActive(bool isActive);

    /**
     * Methods for handling pivots in the trailing submatrix
     */
    // Receive and process batches of pivot operations sent out by
    // the current active diagonal chare
    entry [local] void pivotBatch(int tag) {
      while (pivotBatchTag < (internalStep+1)*blkSize) {
        // Wait for msg carrying the next batch of pivot ops
        when applyTrailingPivots[pivotBatchTag](pivotSequencesMsg *msg) {
          // Send out any pivot data owned by this chare
          atomic "sendOutgoingPivots" {
            sendPendingPivots(msg);
            CmiReference(UsrToEnv(msg));
          }
          // Wait for all incoming pivot data from this batch
          while (pendingIncomingPivots > 0) {
            when trailingPivotRowsSwap[pivotBatchTag](pivotRowsMsg *aMsg)
              atomic "acceptIncomingPivots" {
              for (int i = 0; i < aMsg->nRows; i++) {
                CkAssert(thisIndex.x == aMsg->rowNum[i]/blkSize);
                applySwap(aMsg->rowNum[i] % blkSize, 0,
                          &(aMsg->rows[i * blkSize]), aMsg->rhs[i]);
                pendingIncomingPivots--;
              }
              delete aMsg;
            }
          }
          // Increment the batch tag to the next expected
          atomic {
            pivotBatchTag += msg->numRowsProcessed;
            CmiFree(UsrToEnv(msg));
          }
        }
      }

      atomic { thisProxy[thisIndex].pivotBatchDone(tag); }
    };
    // Indicate a set of rows which need to be swapped
    entry [nokeep] void applyTrailingPivots(pivotSequencesMsg *msg);
    // Exchange pivot row contents
    entry void trailingPivotRowsSwap(pivotRowsMsg *msg);
    entry [local] void pivotBatchDone(int tag);

    // Describes the control flow of above-diagonal blocks after
    // they've completed their prescribed trailing updates.
    entry [local] void factorAboveDiagBlock() {
      // Do one last batch of pivots, from the diagonal on this row
      atomic { thisProxy[thisIndex].pivotBatch(1); }

      // After the associated panel's pivots have all arrived, this
      // block can perform its triangular solve.
      //
      // If the next active panel depends on these results, execute
      // eagerly to avoid delaying the work-exposing critical path.
      when pivotBatchDone[1](int tag) if (thisIndex.y == thisIndex.x + 1) {
        when triangularSolve(blkMsg* m) atomic "triangular_solve" {
          computeU(m->data);
          delete m;
        }
      } else {
        // Otherwise, let the local scheduler direct execution, to
        // let it limit memory consumption and avoid inconveniencing
        // an active panel factorization.
        atomic { updateExecuted = false; }
        while (!updateExecuted) {
          when processComputeU(intptr_t update_ptr) atomic "process_compute_U" {
            Update &update = *(Update *)update_ptr;
            if (localScheduler->shouldExecute()) {
              computeU(update.L);
              localScheduler->updateDone(update_ptr);
              updateExecuted = true;
            } else {
              update.triggered = false;
              localScheduler->updateUntriggered();
            }
          }
        }
      }
      atomic "finish_U" {
        scheduler.releaseActiveColumn(thisIndex.y, thisIndex.x);
        scheduleDownwardU();
        factored = true;
        localScheduler->factorizationDone(thisIndex);
      }
      atomic "abovediag_iteration_completed" { contribute(factorizationDone); }
      // Swallow excess pivots message broadcast to the entire array
      while (true) { when applyTrailingPivots(pivotSequencesMsg *msg) { } }
    };
    // Method for delivering triangular solve data
    entry void triangularSolve(blkMsg* m);

    // These process* entry methods are invoked locally on each block
    // by the BlockScheduler with varying priorities, to let the
    // scheduler decide when the work they represent (the bulk of the
    // computation) should execute.
    entry void processTrailingUpdate(int step, intptr_t update_ptr);
    entry void processComputeU(intptr_t update_ptr);

    // Below diagonal
    entry [local] void factorBelowDiagBlock() {
      atomic { startCALUPivoting(); }
      // below diagonal
      when recvFactoredBlock(CAPivotMsg* msg) atomic {
	size_t rowSize = blkSize * sizeof(double);

	// apply pivots
	for (int i = 0; i < blkSize; ++i) {
	  if (thisIndex.x == msg->rows[i] / blkSize) {
	    unsigned int localRow = msg->rows[i] % blkSize;
	    memcpy(&LU[getIndex(localRow, 0)], msg->data + blkSize*i, rowSize);
	  }
	}

	computeL(msg->data + blkSize * blkSize);
      }
      atomic "below_send_L" {
        if (pendingUmsg) dropRef(pendingUmsg);
        scheduleRightwardL();
        factored = true;
        localScheduler->factorizationDone(thisIndex);
        pivotBatchTag += blkSize;
      }

      while (blockPulled < numBlks-thisIndex.y-1) {
        when blockUsed(int count) atomic { blockPulled += count; }
      }

      for (internalStep++; internalStep <= thisIndex.x; internalStep++) {
        atomic { thisProxy[thisIndex].pivotBatch(2); }
        when pivotBatchDone[2](int tag) { }
      }
      atomic "belowdiag_iteration_completed" { contribute(factorizationDone); }
      // Swallow excess pivots message broadcast to the entire array
      while (true) { when applyTrailingPivots(pivotSequencesMsg *msg) { } }
    };
    // Indicate when this block has been used for remote calculations,
    // to let subsequent pivoting go ahead
    entry void blockUsed(int count);
    entry void recvFactoredBlock(CAPivotMsg* msg);

    entry void factorDiagBlock() {
      atomic {
	ckout << "--------------------------------------------------------------------"<<endl
	      << "Block " << thisIndex.x << " queueing local LU at internalStep "
	      << internalStep << ", start time = " << CmiWallTimer() << ", time = "
	      << CmiWallTimer() - startTime << endl;
	startCALUPivoting();
      }
      when preprocessFinished(CkReductionMsg* msg) atomic {
        CAPivotMsg *pre = getPivotMessage(msg, true);
        CAPivotMsg *Umsg = new (2 * blkSize*blkSize, blkSize) CAPivotMsg(blkSize, pre->rows);
	// Copy in original diagonal data for pivoting
	size_t blockBytes = blkSize*blkSize*sizeof(double);
	memcpy(Umsg->data, LU, blockBytes);

	memcpy(LU, pre->data, blockBytes);

	int info;
	std::vector<int> pivots(blkSize);
	dgetrf(blkSize, blkSize, LU, blkSize, &pivots[0], &info);
	//for (int i = 0; i < blkSize; ++i)
	//  CkAssert(pivots[i] == i);

	memcpy(Umsg->data + blkSize*blkSize, LU, blockBytes);

	activePanel.recvFactoredBlock(Umsg);

	// Store the pivot information
	for (int i = 0; i < blkSize; ++i)
	  recordPivot(thisIndex.y*blkSize + i, pre->rows[i]);
	announceAgglomeratedPivots();
      }

      atomic "diag_multicast_L" {
        ckout << "Block " << thisIndex.x << " finished local LU at internalStep "
              << internalStep << ", time = " << CmiWallTimer() - startTime << endl;
	fflush(stdout);

        localScheduler->factorizationDone(thisIndex);
        factored = true;
        // All diagonal chares except the last must continue the factorization
        if (thisIndex.x < numBlks-1 && thisIndex.y < numBlks-1) {
          blkMsg *msg = new (blkSize * blkSize, 0, mgr->bitsOfPrio()) blkMsg(thisIndex);
          memcpy(msg->data, LU, sizeof(double) * blkSize * blkSize);
          thisProxy(thisIndex.x, thisIndex.y + 1).triangularSolve(msg);
          // Multicast the local block of L rightwards to the blocks in the same row
          scheduleRightwardL();
        }
        // Contribute to a reduction indicating that this chare is
        // done with its factorization work
        contribute(factorizationDone);
      }
    };
    entry void preprocessFinished(CkReductionMsg* msg);

    /**
     * Solve methods
     */
    entry void forwardSolve() {
      if (thisIndex.x > 0) {
        // All except the first diag chare have to wait for their row reductions
        when recvSolveData(int count, double preVec[count]) atomic {
          for (int i = 0; i < blkSize; i++)
            bvec[i] -= preVec[i];
        }
      }
      atomic {
        for (int i = 0; i < blkSize; i++)
          for (int j = 0; j < i; j++)
            bvec[i] -= LU[getIndex(i,j)] * bvec[j];

        if (thisIndex.x != numBlks-1) {
          // Broadcast downward from diagonal
          BVecMsg *m = new (blkSize) BVecMsg(blkSize, bvec, true);
          activePanel.offDiagSolve(m);
        }
        thisProxy(thisIndex).backwardSolve();
      }
    };
    entry void backwardSolve() {
      if (thisIndex.x < numBlks-1) {
        // All except the last diag chare have to wait for their row reductions
        when recvSolveData(int count, double preVec[count]) atomic {
          for (int i = 0; i < blkSize; i++)
            bvec[i] -= preVec[i];
        }
      }

      atomic {
        for (int i = blkSize-1; i >= 0; i--) {
          for (int j = i+1; j < blkSize; j++)
            bvec[i] -= LU[getIndex(i,j)] * bvec[j];
          bvec[i] /= LU[getIndex(i,i)];
        }
        if (thisIndex.x == 0) {
          solveDone.send();
        }
        else {
          // Broadcast upward from diagonal
          CProxySection_LUBlk col =
            CProxySection_LUBlk::ckNew(thisArrayID, 0,           thisIndex.x-1, 1,
                                                    thisIndex.y, thisIndex.y,   1);
          col.ckSectionDelegate(mcastMgr);
          BVecMsg *m = new (blkSize) BVecMsg(blkSize, bvec, false);
          col.offDiagSolve(m);
        }
      }
    };


    /// For off-diagonal blocks, this performs the computations required for fwd and bkwd solves
    entry [nokeep] void offDiagSolve(BVecMsg *m) atomic {
      // Do local portion of solve (daxpy)
      double *xvec = new double[blkSize], *preVec = m->data;
      for (int i = 0; i < blkSize; i++) {
        xvec[i] = 0.0;
        for (int j = 0; j < blkSize; j++)
          xvec[i] += LU[getIndex(i,j)] * preVec[j];
      }

      // Set the diagonal chare on my row as target of reduction
      CkCallback cb(CkReductionTarget(LUBlk, recvSolveData), thisProxy(thisIndex.x, thisIndex.x));
      // Reduce row towards diagonal chare
      mcastMgr->contribute(sizeof(double) * blkSize, xvec, CkReduction::sum_double,
              m->forward ? rowBeforeCookie : rowAfterCookie, cb, thisIndex.x);
      delete[] xvec;
    };

    entry [reductiontarget] void recvSolveData(int count, double data[count]);

    // A remote BlockScheduler wants this block for a triangular solve
    // or trailing update
    entry void requestBlock(int pe, int rx, int ry);

    /**
     * Initialization methods
     */
    entry void startup(const LUConfig luCfg, CProxy_LUMgr _mgr, CProxy_BlockScheduler bs,
                       CkCallback initialized, CkCallback factored, CkCallback solved) {
      when schedulerReady(CkReductionMsg *m) atomic {
	init(luCfg, _mgr, bs, initialized, factored, solved);
      }
      when multicastRedns(int dummy) {
        if (isOnDiagonal() || isBelowDiagonal()) {
          when prepareForPivotRedn(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(pivotCookie, msg);
          }
        }
        if (isBelowDiagonal()) {
          when prepareForRowBeforeDiag(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(rowBeforeCookie, msg);
          }
        } else if (isAboveDiagonal()) {
          when prepareForRowAfterDiag(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(rowAfterCookie, msg);
          }
        }
	when dataReady(int foo) atomic {
	  contribute(initDone);
	}
      }
    };
    entry [nokeep] void schedulerReady(CkReductionMsg *m);
    entry void dataReady(int);
    entry [nokeep] void prepareForPivotRedn(rednSetupMsg *);
    entry [nokeep] void prepareForActivePanel(rednSetupMsg *);
    entry [nokeep] void prepareForRowBeforeDiag(rednSetupMsg *);
    entry [nokeep] void prepareForRowAfterDiag(rednSetupMsg *);
    entry void multicastRedns(int dummy);
  };

  array [1D] BlockScheduler {
    entry BlockScheduler(CProxy_LUBlk luArr, LUConfig config, CProxy_LUMgr mgr_);
    entry void deliverBlock(blkMsg *m);
    entry void printBlockLimit();
    entry void allRegistered(CkReductionMsg *m);
    entry void scheduleSend(CkIndex2D index, bool onActive);
    entry void releaseActiveColumn(const int y, const int t);
    entry void outputStats();
  }
};

module luMessages {
  message BlockReadyMsg;
  message rednSetupMsg;

  message blkMsg {
    double data[];
    int pes[];
  };

  message UMsg {
    double data[];
  };

  message BVecMsg {
    double data[];
  };

  message pivotSequencesMsg {
    int seqIndex[];
    int pivotSequence[];
  };

  message pivotRowsMsg {
    int rowNum[];
    double rows[];
    double rhs[];
  };

  message CAPivotMsg {
    double data[];
    unsigned int rows[];
  };
};
