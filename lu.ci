mainmodule lu {
  readonly CProxy_Main mainProxy;

  readonly ComlibInstanceHandle multicastStats[4];
  readonly int traceTrailingUpdate;
  readonly int traceComputeU;
  readonly int traceComputeL;
  readonly int traceSolveLocalLU;
  readonly CProxy_locker lg;
  readonly CProxy_MemoryMgr memMgr;
//  readonly int memThreshold;

  // Register the reducer function that handles reductions of locval
  initproc void registerLocValReducer();

  message blkMsg {
    double data[];
  };

  message UMsg {
    double data[];
  };

  message pivotMsg;

  message pivotSequencesMsg {
      int seqIndex[];
      int pivotSequence[];
  };

  message pivotRowsMsg {
      int    rowNum[];
      double rows[];
      double rhs[];
  };

  message rednSetupMsg;

  mainchare Main {
    entry Main(CkArgMsg *m);
    entry void finishInit();
    entry void continueIter();
    entry void arrayIsCreated(CkReductionMsg *);
    entry void outputStats();
    entry void calcScaledResidual(CkReductionMsg *);
    entry void iterationCompleted();
  };

  group LUMgr { };

  group PrioLU {
    entry PrioLU(int BLKSIZE, int matSize);
  };

  nodegroup locker {
      entry locker();
  }

  class CProxyElement_LUBlk;

  nodegroup MemoryMgr {
    entry MemoryMgr();
    entry [exclusive] void tryContinue(CProxyElement_LUBlk);
    entry [exclusive] void finishedTrailing(int);
  }

  array [2D] LUBlk {
    entry LUBlk(void);
    entry void prepareForPivotRedn(rednSetupMsg *);
    entry void prepareForActivePanel(rednSetupMsg *);
    entry void prepareForPivotLR(rednSetupMsg *);
    entry void prepareForRowBeforeDiag(rednSetupMsg *);
    entry void prepareForRowAfterDiag(rednSetupMsg *);
    entry void multicastRedns(int dummy);
    entry void initVec();
    entry void startValidation();
    entry [nokeep] void recvXvec(int size, double xvec[size]);
    entry void sumBvec(int size, double partial_b[size]);

    entry void colMax(CkReductionMsg *m);
    entry void sendPivotData(int rowIndex, int size, double data[size], double b);
    entry void doPivot(pivotMsg *msg);
    entry void applyPivots(pivotSequencesMsg *msg);
    entry void acceptPivotData(pivotRowsMsg *msg);
    entry void sendUSegment(UMsg* msg);
    entry void diagUpdate(int);
    entry void continueTrailing(int);

    // Describes the control flow for all (super, sub and on diagonal) LU chares
    entry void factor() {
      //-----------------------------------------------------------------------
      // Each chare in the array has to process these many trailing updates
      // before the diagonal chare on its row / column becomes active in the factorization process
      //-----------------------------------------------------------------------
      atomic { pivotBatchTag = 0; }
      for (internalStep = 0; internalStep < min(thisIndex.x, thisIndex.y); internalStep++) {
          // Receive and process batches of pivot operations sent out by the current active diagonal chare
          while (pivotBatchTag < (internalStep+1)*BLKSIZE) {
              // Wait for msg carrying the next batch of pivot ops
              when applyPivots[pivotBatchTag](pivotSequencesMsg *msg) {
                  // Send out any pivot data owned by this chare
                  atomic "sendOutgoingPivots_trailingChare" { sendPendingPivots(msg); }
                  // Wait for all incoming pivot data from this batch
                  while (pendingIncomingPivots > 0)
                  {
                      when acceptPivotData[pivotBatchTag](pivotRowsMsg *aMsg) atomic "acceptIncomingPivots_trailingChare" {
                          for (int i = 0; i < aMsg->nRows; i++)
                          {
                              CkAssert(thisIndex.x == aMsg->rowNum[i]/BLKSIZE);
                              applySwap(aMsg->rowNum[i]%BLKSIZE, 0, &(aMsg->rows[i*BLKSIZE]), aMsg->rhs[i]);
                              pendingIncomingPivots--;
                              VERY_VERBOSE_PIVOT_AGGLOM("[%d,%d] received pivot row %d for batch %d. %d pending\n",
                                  thisIndex.x, thisIndex.y, aMsg->rowNum[i], pivotBatchTag, pendingIncomingPivots);
                          }
                          delete aMsg;
                      }
                  }
                  // Increment the batch tag to the next expected
                  atomic {
                      pivotBatchTag += msg->numRowsProcessed;
                      delete msg;
                  }
              }
          }

          // Once all pending pivot ops from the current step of the computation are complete,
          // receive the msgs carrying data for the trailing update from the current step

          if (CmiMemoryUsage() > DELETE_MEMORY_THRESHOLD) atomic {
            CkPrintf("pre memory_mode on, CmiMemoryUsage() reporting %f\n", (double)CmiMemoryUsage()/1024.0/1024.0);
            memory_mode = true;
          }

          if (!memory_mode) {
            overlap {
              when recvL[internalStep](blkMsg *mL) atomic {
                if (CmiMemoryUsage() > DELETE_MEMORY_THRESHOLD) {
                  DEBUG_MEM_RESEND("memory_mode on, CmiMemoryUsage() reporting %f\n", (double)CmiMemoryUsage()/1024.0/1024.0);
                  memory_mode = true;
                }
                takeRef(mL);
                L = mL;
              }
              when recvU[internalStep](blkMsg *mU) atomic {
                if (CmiMemoryUsage() > DELETE_MEMORY_THRESHOLD) {
                  DEBUG_MEM_RESEND("memory_mode on, CmiMemoryUsage() reporting %f\n", (double)CmiMemoryUsage()/1024.0/1024.0);
                  memory_mode = true;
                }
                takeRef(mU);
                U = mU;
              }
            }
            // Schedule the trailing update for sometime later
            atomic "trailing_schedule_update" {
              CkEntryOptions opts;
              opts.setPriority(thisIndex.y * BLKSIZE);
              thisProxy(thisIndex.x, thisIndex.y).processTrailingUpdate(internalStep, &opts);
            }
            when processTrailingUpdate[internalStep](int step) atomic "trailing_update" {
              updateMatrix(L, U);
              dropRef(L);
              dropRef(U);
            }
          } else {
            while (__cDep->getMessage(CDEP_MSG_RECVL) != 0) {
              atomic {
                DEBUG_MEM_RESEND("(%d, %d): waiting on message\n", thisIndex.x, thisIndex.y);
              }
              when recvL(blkMsg *mL) atomic {
                DEBUG_MEM_RESEND("(%d, %d): dequeuing L, %d\n", thisIndex.x, thisIndex.y, CkGetRefNum(mL));
                Lqueue.insert(CkGetRefNum(mL));
              }
              atomic {
                DEBUG_MEM_RESEND("(%d, %d): found message\n", thisIndex.x, thisIndex.y);
              }
            }
            while (__cDep->getMessage(CDEP_MSG_RECVU) != 0) {
              atomic {
                DEBUG_MEM_RESEND("(%d, %d): waiting on message\n", thisIndex.x, thisIndex.y);
              }
              when recvU(blkMsg *mU) atomic {
                DEBUG_MEM_RESEND("(%d, %d): dequeuing U, %d\n", thisIndex.x, thisIndex.y, CkGetRefNum(mU));
                Uqueue.insert(CkGetRefNum(mU));
              }
              atomic {
                DEBUG_MEM_RESEND("(%d, %d): found message\n", thisIndex.x, thisIndex.y);
              }
            }
            if (Lqueue.find(internalStep) == Lqueue.end()) {
              atomic {
                DEBUG_MEM_RESEND("(%d, %d): waiting on L, is = %d\n", thisIndex.x, thisIndex.y, internalStep);
              }
              when recvL[internalStep](blkMsg *mL) atomic {
                Lqueue.insert(CkGetRefNum(mL));
              }
              atomic {
                DEBUG_MEM_RESEND("(%d, %d): received L, is = %d\n", thisIndex.x, thisIndex.y, internalStep);
              }
            }
            if (Uqueue.find(internalStep) == Uqueue.end()) {
              atomic {
                DEBUG_MEM_RESEND("(%d, %d): waiting on U, is = %d\n", thisIndex.x, thisIndex.y, internalStep);
              }
              when recvU[internalStep](blkMsg *mU) atomic {
                Uqueue.insert(CkGetRefNum(mU));
              }
              atomic {
                DEBUG_MEM_RESEND("(%d, %d): received U, is = %d\n", thisIndex.x, thisIndex.y, internalStep);
              }
            }
            if (Lqueue.find(internalStep) != Lqueue.end() &&
                Uqueue.find(internalStep) != Uqueue.end()) {
              atomic {
                memMgr[CkMyNode()].tryContinue(thisProxy(thisIndex.x, thisIndex.y));
              }
              when continueTrailing(int dummy) {
                atomic {
                  // Found a matching pair of L and U, now ask for resends
                  Lqueue.erase(internalStep);
                  Uqueue.erase(internalStep);
                  thisProxy(thisIndex.x, internalStep).resendL(thisIndex.x, thisIndex.y);
                  thisProxy(internalStep, thisIndex.y).resendU(thisIndex.x, thisIndex.y);
                  DEBUG_MEM_RESEND("(%d, %d): waiting on resends\n", thisIndex.x, thisIndex.y);
                }
                when recvResendL(blkMsg *mL) atomic {
                  takeRef(mL);
                  L = mL;
                }
                when recvResendU(blkMsg *mU) atomic {
                  takeRef(mU);
                  U = mU;
                }
                atomic {
                  DEBUG_MEM_RESEND("(%d, %d): resends received\n", thisIndex.x, thisIndex.y);
                  updateMatrix(L, U);
                  dropRef(L);
                  dropRef(U);
                  memMgr[CkMyNode()].finishedTrailing(0);
                }
              }
            }
            if (Lqueue.empty() && Uqueue.empty()) {
              atomic {
                memory_mode = false;
              }
            }
            atomic {
              DEBUG_MEM_RESEND("(%d, %d): seaching in queue, internalStep = %d\n", thisIndex.x, thisIndex.y, internalStep);
              DEBUG_MEM_RESEND("Lqueue:");
              for (std::set<int>::iterator it = Lqueue.begin();
                   it != Lqueue.end(); ++it) {
                DEBUG_MEM_RESEND("%d, ", *it);
              }
              DEBUG_MEM_RESEND("\n");
              DEBUG_MEM_RESEND("Uqueue:");
              for (std::set<int>::iterator it = Uqueue.begin();
                   it != Uqueue.end(); ++it) {
                DEBUG_MEM_RESEND("%d, ", *it);
              }
              DEBUG_MEM_RESEND("\n");
            }
          }
        }
      atomic {
        DEBUG_MEM_RESEND("(%d, %d): out of trailing work\n", thisIndex.x, thisIndex.y);
      }

      //-----------------------------------------------------------------------
      // Once the current step of the computation reaches the diagonal chare on your row / column
      //-----------------------------------------------------------------------

      // On chare array diagonal
      if (thisIndex.x == thisIndex.y) atomic {
        ckout << "--------------------------------------------------------------------"<<endl
              <<"Block " << thisIndex.x << " queueing local LU at internalStep " << internalStep<<endl;
        thisProxy(thisIndex.x, thisIndex.y).processLU();
      }
      // Above diagonal
      else if (thisIndex.x < thisIndex.y) {
        // Receive and process each pivot selection sent out by the current diagonal block
        while (pivotBatchTag < (internalStep+1)*BLKSIZE) {
          // Wait for msg carrying the next batch of pivot ops
          when applyPivots[pivotBatchTag](pivotSequencesMsg *msg) {
              // Send out any pivot data owned by this chare
              atomic "sendOutgoingPivots_activeRowChare" { sendPendingPivots(msg); }
              // Wait for all incoming pivot data from this batch
              while (pendingIncomingPivots > 0)
              {
                  when acceptPivotData[pivotBatchTag](pivotRowsMsg *aMsg) atomic "acceptIncomingPivots_activeRowChare" {
                      for (int i = 0; i < aMsg->nRows; i++)
                      {
                          CkAssert(thisIndex.x == aMsg->rowNum[i]/BLKSIZE);
                          applySwap(aMsg->rowNum[i]%BLKSIZE, 0, &(aMsg->rows[i*BLKSIZE]), aMsg->rhs[i]);
                          pendingIncomingPivots--;
                          VERY_VERBOSE_PIVOT_AGGLOM("[%d,%d] received pivot row %d for batch %d. %d pending\n",
                              thisIndex.x, thisIndex.y, aMsg->rowNum[i], pivotBatchTag, pendingIncomingPivots);
                      }
                      delete aMsg;
                  }
              }
              // Increment the batch tag to the next expected
              atomic {
                  pivotBatchTag += msg->numRowsProcessed;
                  delete msg;
              }
          }
        }
        // Once all pivot info is processed, perform the row update
        atomic {
          DEBUG_MEM_RESEND("(%d, %d): waiting for recvL\n", thisIndex.x, thisIndex.y);
        }
        if (!memory_mode) {
          when recvL[internalStep](blkMsg *mL) atomic "beforeL_schedule_computeU" {
            CmiReference(UsrToEnv(mL));
            L = mL;
            CkEntryOptions opts;
            opts.setPriority(thisIndex.y * BLKSIZE);
            thisProxy(thisIndex.x, thisIndex.y).processComputeU(0, &opts);
          }
        } else {
          atomic {
            thisProxy(thisIndex.x, internalStep).resendL(thisIndex.x, thisIndex.y);
          }
          when recvResendL(blkMsg *mL) atomic {
            CmiReference(UsrToEnv(mL));
            L = mL;
            CkEntryOptions opts;
            opts.setPriority(thisIndex.y * BLKSIZE);
            thisProxy(thisIndex.x, thisIndex.y).processComputeU(0, &opts);
          }
        }
      }
      // Below diagonal
      else {
        // Work in the active column of the chare array proceeds column-by-column through the matrix
        for (activeCol = 0; activeCol < BLKSIZE; activeCol++) {
          atomic "below_find_pivot" {
            // Find the local maximum in the active column
            locval l = findLocVal(0, activeCol);

            // Contribute to a reduction along the pivot section to identify the pivot row
            // The pivot section includes all sub-diagonal chares along the active column of the chare array
            mcastMgr->contribute(sizeof(locval), &l, LocValReducer, pivotCookie);
          }
          // Once pivot is identified, and diagonal chare sends out the pivot update
          when doPivot[internalStep*BLKSIZE + activeCol](pivotMsg *msg) {
            atomic "below_recv_pivot" {}
            // Assume that row2 is always the non-diagonal row and that LU is row-major
            if (msg->row2 / BLKSIZE == thisIndex.x) {
              atomic "below_send_remote_pivot" {
                CkEntryOptions opts;
                opts.setPriority(-1);
                thisProxy(thisIndex.y, thisIndex.y)
                .sendPivotData(internalStep*BLKSIZE + activeCol, BLKSIZE, &LU[msg->row2 % BLKSIZE][0], bvec[msg->row2 % BLKSIZE], &opts);
              }
              when sendPivotData[internalStep*BLKSIZE + activeCol](int rowIndex, int size, double data[size], double b) atomic "below_recv_remote_pivot" {
                applySwap(msg->row2 % BLKSIZE, 0, data, b);
              }
            }
          }
          // The diagonal chare also then sends out the post-pivoting row that can be used for updates
          when sendUSegment(UMsg* msg) atomic "below_recv_useg" {
            // Compute the multipliers using the diagonal element
            computeMultipliers(msg->data[0], 0, activeCol);
            // Update all trailing columns in the same block based on the multipliers and Usegment
            updateAllCols(activeCol, msg->data);

            // Options: this SDAG code (or something like it)
            // Somehow this needs to be overlapped with the maxCol sends
            //for (int innercol = 1; innercol < BLKSIZE; innercol++) {
            //thisProxy(thisIndex.x, thisIndex.y).updateRemCols(innercol);
            //}
            //when updateRemCols(int icol) atomic {
            //doUpdate();
            //}
          }
        }
        atomic "below_send_L" {
          multicastRecvL();
          pivotBatchTag += BLKSIZE;
        }

        for (internalStep++;  internalStep <= thisIndex.x; internalStep++) {
          // Receive and process batches of pivot operations sent out by the current active diagonal chare
          while (pivotBatchTag < (internalStep+1)*BLKSIZE) {
              // Wait for msg carrying the next batch of pivot ops
              when applyPivots[pivotBatchTag](pivotSequencesMsg *msg) {
                  // Send out any pivot data owned by this chare
                  atomic "sendOutgoingPivots_leftSectionChare" { sendPendingPivots(msg); }
                  // Wait for all incoming pivot data from this batch
                  while (pendingIncomingPivots > 0)
                  {
                      when acceptPivotData[pivotBatchTag](pivotRowsMsg *aMsg) atomic "acceptIncomingPivots_leftSectionChare" {
                          for (int i = 0; i < aMsg->nRows; i++)
                          {
                              CkAssert(thisIndex.x == aMsg->rowNum[i]/BLKSIZE);
                              applySwap(aMsg->rowNum[i]%BLKSIZE, 0, &(aMsg->rows[i*BLKSIZE]), aMsg->rhs[i]);
                              pendingIncomingPivots--;
                              VERY_VERBOSE_PIVOT_AGGLOM("[%d,%d] received pivot row %d for batch %d. %d pending\n",
                                  thisIndex.x, thisIndex.y, aMsg->rowNum[i], pivotBatchTag, pendingIncomingPivots);
                          }
                          delete aMsg;
                      }
                  }
                  // Increment the batch tag to the next expected
                  atomic {
                      pivotBatchTag += msg->numRowsProcessed;
                      delete msg;
                  }
              }
          }
        }
      }
      if (thisIndex.x != thisIndex.y) {
        atomic "offdiag_iteration_completed" {
          DEBUG_PIVOT("[%d] chare %d,%d contributing\n", CkMyPe(), thisIndex.x, thisIndex.y);
          contribute(CkCallback(CkIndex_Main::iterationCompleted(), mainProxy));
        }
      }
    };

    entry void processLU() {
      atomic {
        if (STOP_AFTER == thisIndex.x) {
            flushLogs();
            CkAbort("Aborting factorization in progress");
        }
        CkAssert(pivotBatchTag == thisIndex.x*BLKSIZE);
        pivotRecords.clear();
        numRowsSinceLastPivotSend = 0;
      }
      // Process matrix column-by-column
      for (activeCol = 0; activeCol < BLKSIZE; ++activeCol) {
        atomic "diag_find_colmax" {
          VERBOSE_PROGRESS(" ..col%d ", thisIndex.x*BLKSIZE + activeCol);
          // Find the local maximum in the active column
          l = findLocVal(activeCol, activeCol);
          // Contribute to a redn along the pivot section to identify the pivot row
          mcastMgr->contribute(sizeof(locval), &l, LocValReducer, pivotCookie);
        }

        // Find the pivot row
        when colMax(CkReductionMsg *m) atomic "diag_found_colmax" {
          l = *(locval *)(m->getData());
        }

        // Share the pivot info with the appropriate array section
        atomic "diag_send_pivot_active" {
          // Send pivot info to all chares that will care (below section)
          pivotMsg *msg = new(sizeof(int)*8) pivotMsg(internalStep*BLKSIZE + activeCol, activeCol + BLKSIZE * thisIndex.y, l.loc);
          *(int*)CkPriorityPtr(msg) = -1;
	  CkSetQueueing(msg, CK_QUEUEING_IFIFO);
          pivotSection.doPivot(msg);
        }

        // If required chunk of pivot row is not with me (but in a chare below me)
        if (l.loc / BLKSIZE != thisIndex.x) {
          when doPivot[internalStep*BLKSIZE + activeCol](pivotMsg *msg) atomic "diag_send_remote_pivot" {
            // Trigger swap of data
            CkEntryOptions opts;
            opts.setPriority(-1);
            thisProxy(msg->row2 / BLKSIZE, thisIndex.y)
            .sendPivotData(internalStep*BLKSIZE + activeCol, BLKSIZE, &LU[msg->row1 % BLKSIZE][0], bvec[msg->row1 % BLKSIZE], &opts);
          }
          when sendPivotData[internalStep*BLKSIZE + activeCol](int rowIndex, int size,
                                                               double data[size], double b) atomic "diag_receive_remote_pivot" {
            applySwap(activeCol, 0, data, b);
          }
        } else {
          when doPivot[internalStep*BLKSIZE + activeCol](pivotMsg *msg) atomic "diag_local_pivot" {
            //DEBUG_PIVOT("swapLocal being called\n");
            swapLocal(l.loc - BLKSIZE * thisIndex.x, activeCol);
          }
        }

        // Now, send the post-diagonal portion of the matrix row (a segment of U)
        atomic "diag_send_useg" {
          UMsg *msg = new(BLKSIZE - activeCol, sizeof(int)*8) UMsg(BLKSIZE - activeCol, &LU[activeCol][activeCol]);
          *(int*)CkPriorityPtr(msg) = -1;
	  CkSetQueueing(msg, CK_QUEUEING_IFIFO);
          activePanel.sendUSegment(msg);
          thisProxy(thisIndex.x, thisIndex.y).diagUpdate(0);
        }

        when diagUpdate(int a) atomic "diag_update" {
          diagonalUpdate(activeCol);
        }

    atomic "diag_agglomerate_pivots" {
        // Store the pivot information
        recordPivot(thisIndex.y*BLKSIZE + activeCol, l.loc);
        // at opportune steps, send out the agglomerated pivot ops
        if ( shouldSendPivots() || activeCol == BLKSIZE-1)
            announceAgglomeratedPivots();
        }
      }

      atomic "diag_multicast_L" {
        // All diagonal chares except the last must continue the factorization
        if (thisIndex.x < numBlks-1 && thisIndex.y < numBlks-1) {
          DEBUG_PRINT("[%d] chare %d,%d is top,left block this step, multicast L\n", CkMyPe(), thisIndex.x, thisIndex.y);
          // Multicast the local block of L rightwards to the blocks in the same row
          multicastRecvL();
        }
        // Contribute to a reduction indicating that this chare is done with its factorization work
        contribute(CkCallback(CkIndex_Main::iterationCompleted(), mainProxy));
        VERBOSE_PROGRESS("\n");
      }
    };

    entry void beginForward(int size, double preVec[size]);
    entry void beginBackward(int size, double preVec[size]);
    entry void recvSolveData(CkReductionMsg *m);

    entry void startup(const LUConfig luCfg, int _whichMulticastStrategy, CProxy_LUMgr _mgr) {
      atomic {
        init(luCfg, _whichMulticastStrategy, _mgr);
      }
      when multicastRedns(int dummy) {
        if (thisIndex.x >= thisIndex.y) {
          when prepareForPivotRedn(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(pivotCookie, msg);
          }
        }
        if (thisIndex.x > thisIndex.y) {
          when prepareForRowBeforeDiag(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(rowBeforeCookie, msg);
          }
          when prepareForActivePanel(rednSetupMsg *msg) atomic { }
        } else if (thisIndex.x < thisIndex.y) {
          when prepareForRowAfterDiag(rednSetupMsg *msg) atomic {
            mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(rowAfterCookie, msg);
          }
        }
        for (ind = 0; ind < thisIndex.x-1; ind++) {
          when prepareForPivotLR(rednSetupMsg *msg) atomic { }
        }
        atomic {
          contribute(CkCallback(CkIndex_Main::finishInit(), mainProxy));
        }
      }
    };

    entry void forwardSolve() {
        if (thisIndex.x > 0) {
          // All except the first diag chare have to wait for their row reductions
          when recvSolveData[thisIndex.x](CkReductionMsg *m) atomic {
            double *preVec = (double*) ( m->getData() );
            for (int i = 0; i < BLKSIZE; i++)
              bvec[i] -= preVec[i];
          }
        }

        atomic {
          localForward(bvec);
          if (thisIndex.x == numBlks-1)
            thisProxy(thisIndex.x, thisIndex.y).backwardSolve();
          else {
            // Broadcast downward from diagonal beginForward
            CProxySection_LUBlk col = CProxySection_LUBlk::ckNew(thisArrayID,
                                                    thisIndex.x+1, numBlks-1, 1,
                                                    thisIndex.y, thisIndex.y, 1);
            col.beginForward(BLKSIZE, bvec);
            thisProxy(thisIndex.x+1,thisIndex.y+1).forwardSolve();
          }
        }
      };

      entry void backwardSolve() {
        if (thisIndex.x < numBlks-1) {
          // All except the last diag chare have to wait for their row reductions
          when recvSolveData[thisIndex.x](CkReductionMsg *m) atomic {
            double *preVec = (double*) ( m->getData() );
            for (int i = 0; i < BLKSIZE; i++)
              bvec[i] -= preVec[i];
          }
        }

        atomic {
          localBackward(bvec);
          if (thisIndex.x == 0)
            mainProxy.iterationCompleted();
          else {
            // Broadcast upward from diagonal beginBackward
            CProxySection_LUBlk col = CProxySection_LUBlk::ckNew(thisArrayID,
                                                    0, thisIndex.x-1, 1,
                                                    thisIndex.y, thisIndex.y, 1);
            col.beginBackward(BLKSIZE, bvec);
            thisProxy(thisIndex.x-1, thisIndex.y-1).backwardSolve();
          }
        }
      };

    // These entry methods are the targets of the row- and column-wise
    // multicasts each block makes.
    //
    // These methods reference but will not modify the messages
    // delivered to them. Thus, we can use the [nokeep] annotation to
    // tell the runtime that it can safely deliver the same message
    // instance to every object on a processor, rather than making a
    // separate copy for each object.
    entry [nokeep] void recvL(blkMsg *);
    entry [nokeep] void recvU(blkMsg *);

    entry [nokeep] void recvResendL(blkMsg *);
    entry [nokeep] void recvResendU(blkMsg *);

    entry void resendL(int, int);
    entry void resendU(int, int);

    // These process* entry methods are invoked locally by each block
    // on itself with varying priorities, to let the scheduler decide
    // when the work they represent (the bulk of the computation)
    // should execute.
    //
    // When a block computes a trailing update, it no longer needs to
    // retain the incoming data from the corresponding row and column
    // updates. The [memcritical] annotation tells the runtime that it
    // should schedule invocations of this entry method when it is in
    // a memory critical (i.e. over threshold) condition, so that
    // retained messages can be freed.
    entry [memcritical] void processTrailingUpdate(int ignoredParam);
    // This frees a buffered message, but don't really reduce
    // memory pressure, because it generates multicasts along the
    // way. Hence, it's not marked [memcritical].
    entry void processComputeU(int ignoredParam);

    entry void flushLogs();
    entry void print();
  };            

  group BlockCyclicMap : CkArrayMap{
    entry BlockCyclicMap();
  };

  group RealBlockCyclicMap : CkArrayMap{
      entry RealBlockCyclicMap(int r, int num_blocks);
  };

  group LUSnakeMap : CkArrayMap{
    entry LUSnakeMap(int, int);
  };

  group LUBalancedSnakeMap : CkArrayMap{
    entry LUBalancedSnakeMap(int, int);
  };

  group LUBalancedSnakeMap2 : CkArrayMap{
    entry LUBalancedSnakeMap2(int, int);
  };

  group PE2DTilingMap: CkArrayMap {
    entry PE2DTilingMap(int _peRows, int _peCols);
  };
};

