module lu {
  extern module luUtils;
  extern module luMessages;

  group StaticBlockSchedule {
    entry StaticBlockSchedule(int numBlks);
  };

  /**
   * 2D array of LU matrix-block objects that are locally coordinated
   * in Structured Dagger.
   *
   * The chare indexing relates to matrix position as follows:
   *           y
   *    o------------>
   *    | (0,0) (0,1) . . .
   *    | (1,0) (1,1)
   *  x |   .         .
   *    |   .           .
   *    |   .             .
   *    v
   */
  array [2D] LUBlk {
    /// Constructor
    entry LUBlk(void);

    /// Each block starts its participation in the factorization here
    entry void factor() {
      atomic {
	fflush(stdout);
	if (started) return;
	started = true;
	//CkPrintf("(%d,%d): scheduleStart()\n", thisIndex.x, thisIndex.y);
	thisProxy(thisIndex.x,thisIndex.y).scheduleStart();
      }
//       // Keep executing trailing updates on this block until it is on
//       // the active block row or column
//       atomic { thisProxy[thisIndex].updateUntilActive(); }
//       // Once the current step of the computation reaches the diagonal
//       // chare on this block's row / column
//       when amActive(bool isActive) {
//         if      (isAboveDiagonal()) atomic { thisProxy[thisIndex].factorAboveDiagBlock(); }
//         else if (isBelowDiagonal()) atomic { thisProxy[thisIndex].factorBelowDiagBlock(); }
//         else     /* on diagonal */  atomic { thisProxy[thisIndex].factorDiagBlock(); }
//       }
//       //when migrateDone(int test) {}
    };

    entry void transition(int dummy);

    entry void scheduleStart() {
      while (currentState != -2) {
	atomic "loadNextState" {
	  StaticBlockSchedule& sched = *(staticProxy.ckLocalBranch());
	  StateNode node = sched.getNextState(thisIndex.x, thisIndex.y, currentState);
	  //CkPrintf("(%d,%d): is = %d, proc = %d, type = %d, task = %d, actualProc = %d\n",
	  //thisIndex.x, thisIndex.y, node.is, node.proc, node.taskType, node.taskid,
	  //CkMyPe());
	  fflush(stdout);
	  switch (node.taskType) {
	  case -1:
	    currentState = -3;
	    break;
	  case 100:
	    thisProxy[thisIndex].trailingUpdateState(node.is, node.taskid, node.proc);
	    break;
	  case 101:
	    thisProxy[thisIndex].processLState(node.is, node.taskid, node.proc);
	    break;
	  case 102:
	    thisProxy[thisIndex].processUState(node.is, node.taskid, node.proc);
	    break;
	  case 103:
	    thisProxy[thisIndex].diagDecomposeState(node.is, node.taskid, node.proc);
	    break;
	  case 104:
	    nproc = node.proc;
	    thisProxy[thisIndex].migrateState();
	    break;
	  }
	  currentState++;
	}
	if (currentState != -2) {
	  when transition(int a) { }
	}
      }
    };

    entry void trailingUpdateState(int is, int taskid, int proc) {
      atomic {
	//CkPrintf("(%d,%d,%d): trailing update taskid = %d, proc = %d\n",
	//thisIndex.x, thisIndex.y, is, taskid, proc);
	fflush(stdout);
	CkAssert(CkMyPe() == proc);
      }
      overlap {
	when recvTrailingL[is](blkMsg* mL) atomic { CmiReference(UsrToEnv(mL)); L = mL; }
	when recvTrailingU[is](blkMsg* mU) atomic { CmiReference(UsrToEnv(mU)); U = mU; }
      }
      atomic "trailingUpdateMain" {
	updateMatrix(LU, LU);
	CmiFree(UsrToEnv(L)); CmiFree(UsrToEnv(U));
      }
      atomic {
	//CkPrintf("(%d,%d,%d): FINISHED trailing update taskid = %d, proc = %d\n",
	//thisIndex.x, thisIndex.y, is, taskid, proc);
	fflush(stdout);
      }
      atomic {
	thisProxy[thisIndex].transition(0);
      }
    };
    
    entry void processUState(int is, int taskid, int proc) {
      atomic {
	//CkPrintf("(%d,%d,%d): process U taskid = %d, proc = %d\n",
	//thisIndex.x, thisIndex.y, is, taskid, proc);
	fflush(stdout);
	CkAssert(CkMyPe() == proc);
      }
      when recvL[is](blkMsg* mL) atomic "processUMain" {
	computeU(LU);
	sendDownwardU();
      }
      atomic {
	//CkPrintf("(%d,%d,%d): FINISHED process U taskid = %d, proc = %d\n",
	//thisIndex.x, thisIndex.y, is, taskid, proc);
	fflush(stdout);
      }
      atomic {
	thisProxy[thisIndex].transition(0);
      }
    };

    entry void processLState(int is, int taskid, int proc) {
      atomic {
	//CkPrintf("(%d,%d,%d): process L taskid = %d, proc = %d\n",
	//thisIndex.x, thisIndex.y, is, taskid, proc);
	fflush(stdout);
	CkAssert(CkMyPe() == proc);
      }
      when recvU[is](blkMsg* mU) atomic "processLMain" {
	computeL(LU);
	sendRightwardL();
      }
      atomic {
	//CkPrintf("(%d,%d,%d): FINISHED process L taskid = %d, proc = %d\n",
	//thisIndex.x, thisIndex.y, is, taskid, proc);
	fflush(stdout);
      }
      atomic {
	thisProxy[thisIndex].transition(0);
      }
    };

    entry void diagDecomposeState(int is, int taskid, int proc) {
      atomic {
	//CkPrintf("(%d,%d,%d): diag decompose taskid = %d, proc = %d\n",
	//thisIndex.x, thisIndex.y, is, taskid, proc);
	fflush(stdout);
	CkAssert(CkMyPe() == proc);
      }
      atomic "diagDecomposeMain" {
	decompose();
	if (thisIndex.x < numBlks - 1) {
	  sendRightwardL();
	  sendDownwardU();
	} else {
	  factorizationDone.send();
	}
      }
      atomic {
	CkPrintf("(%d,%d,%d): FINISHED diag decompose taskid = %d, proc = %d\n",
		 thisIndex.x, thisIndex.y, is, taskid, proc);
	fflush(stdout);
      }
      atomic {
	thisProxy[thisIndex].transition(0);
      }
    };

    entry void migrateState() {
      atomic "migrateStateMain" {
	//CkPrintf("(%d,%d): calling doMigrate nproc = %d, current = %d\n",
	//thisIndex.x, thisIndex.y, nproc, CkMyPe());
	//fflush(stdout);
	thisProxy(thisIndex.x,thisIndex.y).doMigrate(nproc);
      }
      when migrateDone(int dummy) atomic {
	//CkPrintf("(%d,%d): migrate FINISHED current = %d\n", thisIndex.x, thisIndex.y, CkMyPe());
	//fflush(stdout);
      }
      atomic {
	thisProxy[thisIndex].transition(0);
      }
    };

    // Describes the control flow of alternating pivoting and trailing
    // updates for all LUBlk chares before they become part of the
    // active panel.
    entry [local] void updateUntilActive() {
      //-----------------------------------------------------------------------
      // Each chare in the array has to process these many trailing
      // updates before the diagonal chare on its row / column becomes
      // active in the factorization process.
      // -----------------------------------------------------------------------
      atomic {
        startTime = CmiWallTimer();
      }
      for (internalStep = 0; internalStep < min(thisIndex.x, thisIndex.y); internalStep++) {
        overlap {                                      
          when recvTrailingL[internalStep](blkMsg* mL) atomic { CmiReference(UsrToEnv(mL)); L = mL; }
          when recvTrailingU[internalStep](blkMsg* mU) atomic { CmiReference(UsrToEnv(mU)); U = mU; }
        }
        atomic "scheduleTrailing" {
	  CkEntryOptions opts;
	  opts.setPriority(thisIndex.x + thisIndex.y);
          thisProxy(thisIndex.x, thisIndex.y).processTrailingUpdate(internalStep, &opts);
        }
        when processTrailingUpdate[internalStep](int step) atomic "trailingUpdate" {
          ckout << "Block (" << thisIndex.x << "," << thisIndex.y << ") updating at internalStep "
		<< internalStep << ", time = " << CmiWallTimer() - startTime << endl;
	  fflush(stdout);
          updateMatrix(L->data, U->data);
          CmiFree(UsrToEnv(L)); CmiFree(UsrToEnv(U));
        }
      }
      //atomic { thisProxy[thisIndex].amActive(true); }
    };
    //entry void amActive(bool isActive);
    entry void doMigrate(int proc);
    entry void migrateDone(int dummy);

    // Describes the control flow of above-diagonal blocks after
    // they've completed their prescribed trailing updates.
    entry [local] void factorAboveDiagBlock() {
      when recvL[internalStep](blkMsg* mL) atomic {
        CmiReference(UsrToEnv(mL)); L = mL;
	CkEntryOptions opts;
	opts.setPriority(thisIndex.y);
        thisProxy(thisIndex.x, thisIndex.y).processComputeU(0, &opts);
      }
      when processComputeU(int dummy) atomic "processComputeU" {
        ckout << "Block (" << thisIndex.x << "," << thisIndex.y << ") compute U at internalStep "
	      << internalStep << ", time = " << CmiWallTimer() - startTime << endl;
	fflush(stdout);
        computeU(L->data);
        sendDownwardU();
      }
      atomic "abovediag_iteration_completed" { }
    };

    // Below diagonal
    entry [local] void factorBelowDiagBlock() {
      when recvU[internalStep](blkMsg* mU) atomic {
        CmiReference(UsrToEnv(mU)); U = mU;
	CkEntryOptions opts;
	opts.setPriority(thisIndex.x);
        thisProxy(thisIndex.x, thisIndex.y).processComputeL(0, &opts);
      }
      when processComputeL(int dummy) atomic "processComputeL" {
        ckout << "Block (" << thisIndex.x << "," << thisIndex.y << ") compute L at internalStep "
	      << internalStep << ", time = " << CmiWallTimer() - startTime << endl;
	fflush(stdout);
        computeL(U->data);
        sendRightwardL();
      }
      atomic "belowDiagCompleted" {  }
    };

    entry [nokeep] void recvTrailingL(blkMsg*);
    entry [nokeep] void recvTrailingU(blkMsg*);
    entry [nokeep] void recvL(blkMsg*);
    entry [nokeep] void recvU(blkMsg*);

    entry void processTrailingUpdate(int step);
    entry void processComputeU(int dummy);
    entry void processComputeL(int dummy);

    entry void multicastRedns(int);

    entry void factorDiagBlock() {
      atomic "diagDecompose" {
        decompose();
      }
      atomic "diagMulticast" {
        ckout << "Block " << thisIndex.x << " finished local LU at internalStep "
              << internalStep << ", time = " << CmiWallTimer() - startTime << endl;
	fflush(stdout);

        // All diagonal chares except the last must continue the factorization
        if (thisIndex.x < numBlks - 1 && thisIndex.y < numBlks - 1) {
          sendRightwardL();
          sendDownwardU();
        } else {
	  // implies global completion
	  CkPrintf("(%d,%d) complete\n", thisIndex.x, thisIndex.y); fflush(stdout);
	  factorizationDone.send();
	}
        // Contribute to a reduction indicating that this chare is
        // done with its factorization work
	
      }
    };

    /**
     * Solve methods
     */
    entry void forwardSolve() {
      if (thisIndex.x > 0) {
        // All except the first diag chare have to wait for their row reductions
        when recvSolveData(int count, double preVec[count]) atomic {
          for (int i = 0; i < blkSize; i++)
            bvec[i] -= preVec[i];
        }
      }
      atomic {
        for (int i = 0; i < blkSize; i++)
          for (int j = 0; j < i; j++)
            bvec[i] -= LU[getIndex(i,j)] * bvec[j];

        if (thisIndex.x != numBlks-1) {
          // Broadcast downward from diagonal
          BVecMsg *m = new (blkSize) BVecMsg(blkSize, bvec, true);
          activePanel.offDiagSolve(m);
        }
        thisProxy(thisIndex).backwardSolve();
      }
    };
    entry void backwardSolve() {
      if (thisIndex.x < numBlks-1) {
        // All except the last diag chare have to wait for their row reductions
        when recvSolveData(int count, double preVec[count]) atomic {
          for (int i = 0; i < blkSize; i++)
            bvec[i] -= preVec[i];
        }
      }

      atomic {
        for (int i = blkSize-1; i >= 0; i--) {
          for (int j = i+1; j < blkSize; j++)
            bvec[i] -= LU[getIndex(i,j)] * bvec[j];
          bvec[i] /= LU[getIndex(i,i)];
        }
        if (thisIndex.x == 0) {
          solveDone.send();
        }
        else {
          // Broadcast upward from diagonal
          CProxySection_LUBlk col =
            CProxySection_LUBlk::ckNew(thisArrayID, 0,           thisIndex.x-1, 1,
                                                    thisIndex.y, thisIndex.y,   1);
          col.ckSectionDelegate(mcastMgr);
          BVecMsg *m = new (blkSize) BVecMsg(blkSize, bvec, false);
          col.offDiagSolve(m);
        }
      }
    };

    /// For off-diagonal blocks, this performs the computations required for fwd and bkwd solves
    entry [nokeep] void offDiagSolve(BVecMsg *m) atomic {
      // Do local portion of solve (daxpy)
      double *xvec = new double[blkSize], *preVec = m->data;
      for (int i = 0; i < blkSize; i++) {
        xvec[i] = 0.0;
        for (int j = 0; j < blkSize; j++)
          xvec[i] += LU[getIndex(i,j)] * preVec[j];
      }

      // Set the diagonal chare on my row as target of reduction
      CkCallback cb(CkReductionTarget(LUBlk, recvSolveData), thisProxy(thisIndex.x, thisIndex.x));
      // Reduce row towards diagonal chare
      mcastMgr->contribute(sizeof(double) * blkSize, xvec, CkReduction::sum_double,
              m->forward ? rowBeforeCookie : rowAfterCookie, cb, thisIndex.x);
      delete[] xvec;
    };

    entry [reductiontarget] void recvSolveData(int count, double data[count]);

    entry [nokeep] void prepareForActivePanel(rednSetupMsg *);
    entry [nokeep] void prepareForRowBeforeDiag(rednSetupMsg *);
    entry [nokeep] void prepareForRowAfterDiag(rednSetupMsg *);

    entry void dataReady(int);

    /**
     * Initialization methods
     */
    entry void startup(const LUConfig luCfg, CProxy_LUMgr _mgr, CProxy_BlockScheduler bs,
                       CkCallback initialized, CkCallback factored, CkCallback solved,
		       CProxy_StaticBlockSchedule staticProxy) {
      if (!started) {
	when schedulerReady(CkReductionMsg *m) atomic {
	  init(luCfg, _mgr, bs, initialized, factored, solved, staticProxy);
	}
	when multicastRedns(int dummy) {
	  if (isBelowDiagonal()) {
	    when prepareForRowBeforeDiag(rednSetupMsg *msg) atomic {
	      mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
	      CkGetSectionInfo(rowBeforeCookie, msg);
	    }
	  } else if (isAboveDiagonal()) {
	    when prepareForRowAfterDiag(rednSetupMsg *msg) atomic {
	      mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
            CkGetSectionInfo(rowAfterCookie, msg);
	    }
	  }
	  when dataReady(int foo) atomic {
	    contribute(initDone);
	  }
	}
      }
    };
    entry [nokeep] void schedulerReady(CkReductionMsg *m);
  };

  array [1D] BlockScheduler {
    entry BlockScheduler(CProxy_LUBlk luArr, LUConfig config, CProxy_LUMgr mgr_);
    entry void allRegistered(CkReductionMsg *m);
    entry void outputStats();
  }
};

module luMessages {
  message rednSetupMsg;

  message blkMsg {
    double data[];
  };

  message BVecMsg {
    double data[];
  };
};
