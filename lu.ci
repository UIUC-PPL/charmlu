mainmodule lu {
  readonly CProxy_Main mainProxy;

  readonly ComlibInstanceHandle multicastStats[4];
  readonly int traceTrailingUpdate;
  readonly int traceComputeU;
  readonly int traceComputeL;
  readonly int traceSolveLocalLU;
  readonly bool doPrioritize;
//  readonly int memThreshold;

  // Register the reducer function that handles reductions of locval
  initproc void registerLocValReducer();

  message blkMsg {
    double data[];
  };

  message rednSetupMsg;

  mainchare Main {
    entry Main(CkArgMsg *m);
    entry void finishInit();
    entry void continueIter();
    entry void arrayIsCreated(CkReductionMsg *);
    entry void outputStats();
    entry void calcScaledResidual(CkReductionMsg *);
    entry void done(pathInformationMsg *m);
    entry void iterationCompleted();
  };

  group LUMgr { };

  group PrioLU {
    entry PrioLU(int BLKSIZE, int matSize);
  };

  array [2D] LUBlk {
    entry LUBlk(void);
    entry void prepareForPivotRedn(rednSetupMsg *);
    entry void prepareForRowBeforeDiag(rednSetupMsg *);
    entry void prepareForRowAfterDiag(rednSetupMsg *);
    entry void init(int whichMulticastStrategy, int, int, int memThreshold, CProxy_LUMgr);
    entry void initVec();
    entry void startValidation();
    entry [nokeep] void recvXvec(int size, double xvec[size]);
    entry void sumBvec(int size, double partial_b[size]);

    entry void colMax(CkReductionMsg *m);
    entry void sendPivotData(int toGlobalRow, int fromOrigRow, int size, double data[size], double b);
    entry void doPivot(int activeRow, int row1, int row2);
    entry void sendUSegment(int size, double usegment[size]);
    entry void sendPivotDataToFinalOwner(int requestingChareIdx, int nRows, int rowIndex[nRows]);
    entry void assemblePivotedB(int originalOwnerIdx, int nRows, double bChunk[nRows]);

    // Describes the control flow for all (super, sub and on diagonal) LU chares
    entry void factor() {
      //-----------------------------------------------------------------------
      // Each chare in the array has to process these many trailing updates
      // before the diagonal chare on its row / column becomes active in the factorization process
      //-----------------------------------------------------------------------
      for (internalStep = 0; internalStep < min(thisIndex.x, thisIndex.y); internalStep++) {
        // Receive and process each pivot selection sent out by the current diagonal block
        for (pivotBlk = 0; pivotBlk < BLKSIZE; pivotBlk++) {
          when doPivot[internalStep*BLKSIZE + pivotBlk](int step, int row1, int row2) atomic {
            if (row1 != row2) {
              DEBUG_PIVOT("(%d, %d): TRAILING received doPivot for %d and %d, step = %d, tag = %d\n", thisIndex.x, thisIndex.y, row1, row2, internalStep, step);
            }
            doPivotLocal(row1, row2);
          }
          if (remoteSwap) {
            atomic {
              thisProxy(otherRowIndex, thisIndex.y)
              .sendPivotData(globalOtherRow, permutationVec[thisLocalRow], BLKSIZE, &LU[getIndex(thisLocalRow, 0)], bvec[thisLocalRow]);
            }
            when sendPivotData[globalThisRow](int index, int fromOrigRow, int size, double data[size], double b) atomic {
              applySwap(thisLocalRow, fromOrigRow, 0, data, b);
            }
          }
        }

        // Once all pending pivot ops from the current step of the computation are complete,
        // receive the msgs carrying data for the trailing update from the current step
        overlap {
          when recvL[internalStep](blkMsg *mL) atomic
            { CmiReference(UsrToEnv(mL)); L = mL; }
          when recvU[internalStep](blkMsg *mU) atomic
            { CmiReference(UsrToEnv(mU)); U = mU; }
        }
        // Schedule the trailing update for sometime later
        atomic {
          thisProxy(thisIndex.x, thisIndex.y).processTrailingUpdate(internalStep);
        }
        when processTrailingUpdate[internalStep](int step) atomic {
          updateMatrix(L, U);
          CmiFree(UsrToEnv(L));
          CmiFree(UsrToEnv(U));
        }
      }

      //-----------------------------------------------------------------------
      // Once the current step of the computation reaches the diagonal chare on your row / column
      //-----------------------------------------------------------------------

      // On chare array diagonal
      if (thisIndex.x == thisIndex.y) atomic {
        ckout << "Block " << thisIndex.x << " queueing local LU at internalStep " << internalStep<<endl;
        thisProxy(thisIndex.x, thisIndex.y).processLU();
      }
      // Above diagonal
      else if (thisIndex.x < thisIndex.y) {
        // Receive and process each pivot selection sent out by the current diagonal block
        for (pivotBlk = 0; pivotBlk < BLKSIZE; pivotBlk++) {
          when doPivot[internalStep*BLKSIZE + pivotBlk](int step, int row1, int row2) atomic {
            if (row1 != row2) {
              DEBUG_PIVOT("(%d, %d): BEFOREL received doPivot for %d and %d, step = %d, tag = %d\n", thisIndex.x, thisIndex.y, row1, row2, internalStep, step);
            }
            doPivotLocal(row1, row2);
          }
          if (remoteSwap) {
            atomic {
              thisProxy(otherRowIndex, thisIndex.y)
              .sendPivotData(globalOtherRow, permutationVec[thisLocalRow], BLKSIZE, &LU[getIndex(thisLocalRow, 0)], bvec[thisLocalRow]);
            }
            when sendPivotData[globalThisRow](int index, int fromOrigRow, int size, double data[size], double b) atomic {
              applySwap(thisLocalRow, fromOrigRow, 0, data, b);
            }
          }
        }
        // Once all pivot info is processed, perform the row update
        when recvL[internalStep](blkMsg *mL) atomic {
          CmiReference(UsrToEnv(mL));
          L = mL;
          thisProxy(thisIndex.x, thisIndex.y).processComputeU(0);
        }
      }
      // Below diagonal
      else {
        // Work in the active column of the chare array proceeds column-by-column through the matrix
        for (activeCol = 0; activeCol < BLKSIZE; activeCol++) {
          atomic {
            // Find the local maximum in the active column
            locval l = findLocVal(0, activeCol);

            // Contribute to a reduction along the pivot section to identify the pivot row
            // The pivot section includes all sub-diagonal chares along the active column of the chare array
            mcastMgr->contribute(sizeof(locval), &l, LocValReducer, pivotCookie);
          }
          // Once pivot is identified, and diagonal chare sends out the pivot update
          when doPivot[internalStep*BLKSIZE + activeCol](int step, int row1, int row2) {
            atomic {
              if (row1 != row2) {
                DEBUG_PIVOT("(%d, %d): BELOW received doPivot for %d and %d, step = %d, tag = %d\n", thisIndex.x, thisIndex.y, row1, row2, internalStep, step);
              }
            }
            // Assume that row2 is always the non-diagonal row and that LU is row-major
            if (row2 / BLKSIZE == thisIndex.x) {
              atomic {
                thisProxy(thisIndex.y, thisIndex.y)
                .sendPivotData(row2, permutationVec[row2 % BLKSIZE], BLKSIZE, &LU[getIndex(row2 % BLKSIZE, 0)], bvec[row2 % BLKSIZE]);
              }
              when sendPivotData[row1](int rowIndex, int fromOrigRow, int size, double data[size], double b) atomic {
                applySwap(row2 % BLKSIZE, fromOrigRow, 0, data, b);
              }
            }
          }
          // The diagonal chare also then sends out the post-pivoting row that can be used for updates
          when sendUSegment(int size, double usegment[size]) atomic {
            // Compute the multipliers using the diagonal element
            computeMultipliers(usegment[0], 0, activeCol);
            // Update all trailing columns in the same block based on the multipliers and Usegment
            updateAllCols(activeCol, usegment);

            // Options: this SDAG code (or something like it)
            // Somehow this needs to be overlapped with the maxCol sends
            //for (int innercol = 1; innercol < BLKSIZE; innercol++) {
            //thisProxy(thisIndex.x, thisIndex.y).updateRemCols(innercol);
            //}
            //when updateRemCols(int icol) atomic {
            //doUpdate();
            //}
          }
        }
        atomic {
          multicastRecvL();
          currentStep = internalStep+1;
        }
        for (ind = 0;  ind < thisIndex.x - thisIndex.y; ind++) {
          atomic {
            DEBUG_PRINT("(%d, %d): incrementing step, times = %d\n", thisIndex.x, thisIndex.y, ind);
          }
          for (pivotBlk = 0; pivotBlk < BLKSIZE; pivotBlk++) {
            when doPivot[currentStep*BLKSIZE + pivotBlk](int step, int row1, int row2) atomic {
              if (row1 != row2) {
                DEBUG_PIVOT("(%d, %d): LATER received doPivot for %d and %d, step = %d, tag = %d\n", thisIndex.x, thisIndex.y, row1, row2, currentStep, step);
              }
              doPivotLocal(row1, row2);
            }
            if (remoteSwap) {
              atomic {
                thisProxy(otherRowIndex, thisIndex.y)
                .sendPivotData(globalOtherRow, permutationVec[thisLocalRow], BLKSIZE, &LU[getIndex(thisLocalRow, 0)], bvec[thisLocalRow]);
              }
              when sendPivotData[globalThisRow](int index, int fromOrigRow, int size, double data[size], double b) atomic {
                applySwap(thisLocalRow, fromOrigRow, 0, data, b);
              }
            }
          }
          atomic {
            currentStep++;
          }
        }
      }
      if (thisIndex.x != thisIndex.y) {
        atomic {
          DEBUG_PIVOT("[%d] chare %d,%d contributing\n", CkMyPe(), thisIndex.x, thisIndex.y);
          contribute(CkCallback(CkIndex_Main::iterationCompleted(), mainProxy));
        }
      }
    };

    entry void processLU() {
      // Process matrix column-by-column
      for (activeCol = 0; activeCol < BLKSIZE; ++activeCol) {

        // Find the pivot row
        if (thisIndex.x < numBlks - 1) {
          when colMax(CkReductionMsg *m) atomic {
            locval &remoteL = *(locval *)(m->getData());
            l = findLocVal(activeCol, activeCol, remoteL);
          }
        } else atomic {
          l = findLocVal(activeCol, activeCol);
        }

        // Share the pivot info with the appropriate array section
        atomic {
          // Builds a section including all chares on and below the current diagonal element's row
          CProxySection_LUBlk below = CProxySection_LUBlk::ckNew(thisArrayID, thisIndex.x, numBlks-1, 1, 0, numBlks-1, 1);
          // Send pivot info to all chares that will care (below section)
          below.doPivot(internalStep*BLKSIZE + activeCol, activeCol + BLKSIZE * thisIndex.y, l.loc);
          DEBUG_PIVOT("(%d, %d): diagonal sending doPivot tag = %d to pivot %d and %d\n",
          thisIndex.x, thisIndex.y, internalStep*BLKSIZE + activeCol, activeCol + BLKSIZE * thisIndex.y, l.loc);
        }

        // If required chunk of pivot row is not with me (but in a chare below me)
        if (l.loc / BLKSIZE != thisIndex.x) {
          when doPivot[internalStep*BLKSIZE + activeCol](int step, int row1, int row2) atomic {
            // Trigger swap of data
            thisProxy(row2 / BLKSIZE, thisIndex.y)
            .sendPivotData(row1, permutationVec[row1 % BLKSIZE], BLKSIZE, &LU[getIndex(row1 % BLKSIZE, 0)], bvec[row1 % BLKSIZE]);
          }
          when sendPivotData[l.loc](int rowIndex, int fromOrigRow, int size, 
                                    double data[size], double b) atomic {
            applySwap(activeCol, fromOrigRow, 0, data, b);
          }
        } else {
          atomic {
            DEBUG_PIVOT("(%d, %d): diagonal waiting on doPivot tag = %d\n", thisIndex.x, thisIndex.y,
            internalStep*BLKSIZE + activeCol);
          }
          when doPivot[internalStep*BLKSIZE + activeCol](int step, int row1, int row2) atomic {
            //DEBUG_PIVOT("swapLocal being called\n");
            swapLocal(l.loc - BLKSIZE * thisIndex.x, activeCol);
          }
        }

        // Now, send the post-diagonal portion of the matrix block (a segment of U)
        atomic {
          CProxySection_LUBlk oneCol = CProxySection_LUBlk::ckNew(thisArrayID, thisIndex.x+1, numBlks-1, 1, thisIndex.y, thisIndex.y, 1);
          oneCol.sendUSegment(BLKSIZE - activeCol, &LU[getIndex(activeCol, activeCol)]);
          diagonalUpdate(activeCol);
        }
      }

      atomic {
        // Print the final, post-pivoting global row numbers that belong to this chare
        for (int i=0; i< BLKSIZE; i++)
            DEBUG_IMPLICIT_PIVOT("Current row %d was originally row %d\n",thisIndex.x*BLKSIZE + i, permutationVec[i]);
        // Trigger the requests for remote data (b vector pivoting)
        requestRemotePivotData();
        // Record how many rows of pivot data this chare will be sending/recving
        numPendingPivotSends = numRemoteSwaps;
        numPendingPivotRecvs = numRemoteSwaps;
      }

      atomic {
        // All diagonal chares except the last must continue the factorization
        if (thisIndex.x < numBlks-1 && thisIndex.y < numBlks-1) {
          DEBUG_PRINT("[%d] chare %d,%d is top,left block this step, multicast L\n", CkMyPe(), thisIndex.x, thisIndex.y);
          // Multicast the local block of L rightwards to the blocks in the same row
          multicastRecvL();
        }
        // Contribute to a reduction indicating that this chare is done with its factorization work
        contribute(CkCallback(CkIndex_Main::iterationCompleted(), mainProxy));
      }
    };

    entry void beginForward(int size, double preVec[size]);
    entry void beginBackward(int size, double preVec[size]);
    entry void recvSolveData(CkReductionMsg *m);

    entry void multicastRedns() {
      if (thisIndex.x > thisIndex.y) {
        when prepareForPivotRedn(rednSetupMsg *msg) atomic {
          mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
          CkGetSectionInfo(pivotCookie, msg);
        }
        when prepareForRowBeforeDiag(rednSetupMsg *msg) atomic {
          mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
          CkGetSectionInfo(rowBeforeCookie, msg);
        }
      } else if (thisIndex.x < thisIndex.y) {
        when prepareForRowAfterDiag(rednSetupMsg *msg) atomic {
          mcastMgr = CProxy_CkMulticastMgr(msg->rednMgrGID).ckLocalBranch();
          CkGetSectionInfo(rowAfterCookie, msg);
        }
      }
      atomic {
        contribute(CkCallback(CkIndex_Main::finishInit(), mainProxy));
      }
    };

    entry void forwardSolve() {
        if (thisIndex.x > 0) {
          // All except the first diag chare have to wait for their row reductions
          when recvSolveData[thisIndex.x](CkReductionMsg *m) atomic {
            double *preVec = (double*) ( m->getData() );
            for (int i = 0; i < BLKSIZE; i++)
              bvec[i] -= preVec[i];
          }
        }

        atomic {
          localForward(bvec);
          if (thisIndex.x < numBlks-1) {
            // Broadcast downward from diagonal beginForward
            CProxySection_LUBlk col = CProxySection_LUBlk::ckNew(thisArrayID,
                                                    thisIndex.x+1, numBlks-1, 1,
                                                    thisIndex.y, thisIndex.y, 1);
            col.beginForward(BLKSIZE, bvec);
            thisProxy(thisIndex.x+1,thisIndex.y+1).forwardSolve();
          }
        }

        // While there are pivot rows yet to be received / sent out
        overlap {
          while (numPendingPivotSends > 0) {
            // Send out pivot data
            when sendPivotDataToFinalOwner(int requestingChareIdx, int nRows, int rowIndex[nRows]) atomic {
              DEBUG_IMPLICIT_PIVOT("[%d,%d] Sending to chare (%d,%d) b vector rows: ", 
                                   thisIndex.x, thisIndex.y, requestingChareIdx, requestingChareIdx);
              double *bChunk = new double[nRows];
              // Extract the requested rows into a buffer
              for (int i=0; i < nRows; i++) {
                CkAssert(rowIndex[i] / BLKSIZE == thisIndex.x);
                bChunk[i] = bvec[ rowIndex[i]%BLKSIZE ];
                DEBUG_IMPLICIT_PIVOT(" %d ",rowIndex[i]);
              }
              DEBUG_IMPLICIT_PIVOT("\n");
              // Send the required data to the chare that asked for it
              thisProxy(requestingChareIdx, requestingChareIdx).assemblePivotedB(thisIndex.x, nRows, bChunk);
              // Update the number of rows that still need to be sent out
              numPendingPivotSends -= nRows;
              delete bChunk;
            }
          }

          while (numPendingPivotRecvs > 0) {
            // Receive pivot data
            when assemblePivotedB(int originalOwnerIdx, int nRows, double bChunk[nRows]) atomic {
              // Assemble the pivoted data
              assembleB(originalOwnerIdx, nRows, bChunk);
              // Update the number of remote rows yet to be received
              numPendingPivotRecvs -= nRows;
            }
          }
        }

        atomic {
          // Spit out the pivoted y vector
          for (int i=0; i<BLKSIZE; i++)
            DEBUG_IMPLICIT_PIVOT("y[%d] after pivoting = %f\n", thisIndex.x*BLKSIZE+i, pivotedB[i]);
          // Enqueue the backward solve method as pivoting dependencies have been met
          thisProxy(thisIndex.x, thisIndex.y).backwardSolve();
        }
      };

      entry void backwardSolve() {
        if (thisIndex.x < numBlks-1) {
          // All except the last diag chare have to wait for their row reductions
          when recvSolveData[thisIndex.x](CkReductionMsg *m) atomic {
            double *preVec = (double*) ( m->getData() );
            for (int i = 0; i < BLKSIZE; i++)
              bvec[i] -= preVec[i];
          }
        }

        atomic {
          localBackward(bvec);
          if (thisIndex.x == 0)
            mainProxy.iterationCompleted();
          else {
            // Broadcast upward from diagonal beginBackward
            CProxySection_LUBlk col = CProxySection_LUBlk::ckNew(thisArrayID,
                                                    0, thisIndex.x-1, 1,
                                                    thisIndex.y, thisIndex.y, 1);
            col.beginBackward(BLKSIZE, bvec);
          }
        }
      };

    // These entry methods are the targets of the row- and column-wise
    // multicasts each block makes.
    //
    // These methods reference but will not modify the messages
    // delivered to them. Thus, we can use the [nokeep] annotation to
    // tell the runtime that it can safely deliver the same message
    // instance to every object on a processor, rather than making a
    // separate copy for each object.
    entry [nokeep] void recvL(blkMsg *);
    entry [nokeep] void recvU(blkMsg *);

    // These process* entry methods are invoked locally by each block
    // on itself with varying priorities, to let the scheduler decide
    // when the work they represent (the bulk of the computation)
    // should execute.
    //
    // When a block computes a trailing update, it no longer needs to
    // retain the incoming data from the corresponding row and column
    // updates. The [memcritical] annotation tells the runtime that it
    // should schedule invocations of this entry method when it is in
    // a memory critical (i.e. over threshold) condition, so that
    // retained messages can be freed.
    entry [memcritical] void processTrailingUpdate(int ignoredParam);
    // This frees a buffered message, but don't really reduce
    // memory pressure, because it generates multicasts along the
    // way. Hence, it's not marked [memcritical].
    entry void processComputeU(int ignoredParam);

    entry void flushLogs();
    entry void print();
  };            

  group BlockCyclicMap : CkArrayMap{
    entry BlockCyclicMap();
  };

  group LUSnakeMap : CkArrayMap{
    entry LUSnakeMap(int, int);
  };

  group LUBalancedSnakeMap : CkArrayMap{
    entry LUBalancedSnakeMap(int, int);
  };

  group LUBalancedSnakeMap2 : CkArrayMap{
    entry LUBalancedSnakeMap2(int, int);
  };
};
